{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 4 — Image-to-Text with Tesseract OCR\n",
    "Objectives\n",
    "- Install/verify Tesseract OCR.\n",
    "- Extract text from images (JPG/PNG) and scanned PDFs.\n",
    "- Clean and normalize OCR output for better downstream performance.\n",
    "- Chunk and add OCR outputs to a FAISS vector store (extend Module 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies\n",
    "- Native: Tesseract OCR binary (required)\n",
    "- macOS: brew install tesseract\n",
    "- Ubuntu/Debian: sudo apt-get install tesseract-ocr\n",
    "- Windows: Install from https://github.com/tesseract-ocr/tesseract and note the install path.\n",
    "- Python: pytesseract, pillow, pymupdf (for scanned PDFs), sentence-transformers, faiss-cpu, tqdm\n",
    "If needed, install Python libs:\n",
    "%pip install -q pillow pytesseract pymupdf sentence-transformers faiss-cpu tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.9 | FAISS ok | PyMuPDF ok\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import pytesseract\n",
    "from tqdm import tqdm\n",
    "import fitz  # PyMuPDF\n",
    "try:\n",
    "    import faiss  # noqa: F401\n",
    "except Exception:\n",
    "    import faiss_cpu as faiss  # type: ignore\n",
    "print(f\"Python: {sys.version.split()[0]} | FAISS ok | PyMuPDF ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tesseract setup and validation\n",
    "- If on Windows, set pytesseract.pytesseract.tesseract_cmd to the installed path, e.g.:\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "- We validate availability by fetching the version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesseract version: 5.5.0.20241111\n"
     ]
    }
   ],
   "source": [
    "TESSERACT_CMD = os.getenv(\"TESSERACT_CMD\", r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\")  # fill if needed\n",
    "if TESSERACT_CMD:\n",
    "    pytesseract.pytesseract.tesseract_cmd = TESSERACT_CMD\n",
    "try:\n",
    "    print(\"Tesseract version:\", pytesseract.get_tesseract_version())\n",
    "except Exception as e:\n",
    "    print(\"ERROR: Tesseract is not found. Install it and/or set TESSERACT_CMD.\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration\n",
    "- INPUT_IMG_DIR: folder of JPG/PNG images to OCR\n",
    "- INPUT_PDF_DIR: folder of scanned PDFs (or PDFs you want to force OCR)\n",
    "- OUTPUT_DIR: where JSONL of OCR chunks will be written\n",
    "- OCR settings: language(s), OEM/PSM, DPI for PDF rasterization\n",
    "- Chunking: max_chars and overlap; matches downstream RAG usage\n",
    "- Index: path to existing FAISS index (from Module 3) or create new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_IMG_DIR = Path(\"./../data/images\")\n",
    "INPUT_PDF_DIR = Path(\"./../data/scanned_pdfs\")\n",
    "OUTPUT_DIR = Path(\"./../data/ocr_outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OCR_LANG = \"eng\"\n",
    "OCR_OEM = 3\n",
    "OCR_PSM = 3\n",
    "PDF_DPI = 300\n",
    "MAX_CHARS = 1200\n",
    "OVERLAP = 200\n",
    "INDEX_DIR = Path(\"./../data/indexes/my_corpus\")\n",
    "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EMBEDDING_BACKEND = \"gemini\"  # \"minilm\" or \"gemini\"\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding backends (reuse Module 3 approach)\n",
    "- MiniLM (local, default)\n",
    "- Gemini (optional, requires GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Gemini backend: text-embedding-004, dim=768\n"
     ]
    }
   ],
   "source": [
    "class MiniLMBackend:\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        import torch\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "        self.dim = self.model.get_sentence_embedding_dimension()\n",
    "        self.model_name = model_name\n",
    "    def encode(self, texts: List[str], batch_size: int = 64) -> np.ndarray:\n",
    "        return self.model.encode(texts, batch_size=batch_size, convert_to_numpy=True, normalize_embeddings=False)\n",
    "\n",
    "class GeminiBackend:\n",
    "    def __init__(self, model_name: str = \"text-embedding-004\", api_key: Optional[str] = None):\n",
    "        assert api_key, \"GOOGLE_API_KEY required for Gemini embeddings.\"\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.genai = genai\n",
    "        self.dim = 768\n",
    "        self.model_name = model_name\n",
    "    def encode(self, texts: List[str], batch_size: int = 16) -> np.ndarray:\n",
    "        vecs: List[List[float]] = []\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Gemini embedding\"):\n",
    "            for t in texts[i:i+batch_size]:\n",
    "                resp = self.genai.embed_content(model=self.model_name, content=t[:4000])\n",
    "                vecs.append(resp[\"embedding\"])\n",
    "        return np.asarray(vecs, dtype=np.float32)\n",
    "\n",
    "def get_backend(name: str):\n",
    "    if name.lower() == \"minilm\":\n",
    "        be = MiniLMBackend()\n",
    "        print(f\"Using MiniLM backend: {be.model_name}, dim={be.dim}\")\n",
    "        return be\n",
    "    elif name.lower() == \"gemini\":\n",
    "        be = GeminiBackend(api_key=GOOGLE_API_KEY)\n",
    "        print(f\"Using Gemini backend: text-embedding-004, dim={be.dim}\")\n",
    "        return be\n",
    "    else:\n",
    "        raise ValueError(\"EMBEDDING_BACKEND must be 'minilm' or 'gemini'.\")\n",
    "\n",
    "backend = get_backend(EMBEDDING_BACKEND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image preprocessing helpers\n",
    "- Convert to grayscale\n",
    "- Optional upscaling\n",
    "- Contrast/threshold (simple binary threshold)\n",
    "- Deskew using Tesseract OSD (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "def to_grayscale(img: Image.Image) -> Image.Image:\n",
    "    return ImageOps.grayscale(img)\n",
    "def upscale(img: Image.Image, factor: float = 1.5) -> Image.Image:\n",
    "    if factor == 1.0:\n",
    "        return img\n",
    "    w, h = img.size\n",
    "    return img.resize((int(w*factor), int(h*factor)), Image.LANCZOS)\n",
    "def binarize(img: Image.Image, thresh: int = 180) -> Image.Image:\n",
    "    return img.point(lambda p: 255 if p > thresh else 0)\n",
    "def deskew(img: Image.Image) -> Image.Image:\n",
    "    try:\n",
    "        osd = pytesseract.image_to_osd(img)\n",
    "        m = re.search(r\"Rotate: (\\d+)\", osd)\n",
    "        if m:\n",
    "            angle = int(m.group(1))\n",
    "            if angle != 0:\n",
    "                return img.rotate(-angle, expand=True, fillcolor=\"white\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return img\n",
    "def preprocess_image(img: Image.Image, upscale_factor: float = 1.5, use_binarize: bool = True, do_deskew: bool = True) -> Image.Image:\n",
    "    x = to_grayscale(img)\n",
    "    x = upscale(x, factor=upscale_factor)\n",
    "    if do_deskew:\n",
    "        x = deskew(x)\n",
    "    if use_binarize:\n",
    "        x = binarize(x, thresh=180)\n",
    "    x = x.filter(ImageFilter.MedianFilter(size=3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OCR functions\n",
    "- image_to_text: OCR a PIL image to text and average confidence\n",
    "- pdf_page_to_image: rasterize a PDF page at the configured DPI\n",
    "- ocr_image_path: end-to-end for an image file\n",
    "- ocr_pdf_path: end-to-end for a PDF (all pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_text(img: Image.Image, lang: str = OCR_LANG, oem: int = OCR_OEM, psm: int = OCR_PSM) -> Tuple[str, float]:\n",
    "    config = f\"--oem {oem} --psm {psm}\"\n",
    "    txt = pytesseract.image_to_string(img, lang=lang, config=config)\n",
    "    try:\n",
    "        data = pytesseract.image_to_data(img, lang=lang, config=config, output_type=pytesseract.Output.DICT)\n",
    "        confs = [int(c) for c in data.get(\"conf\", []) if c not in (\"-1\", -1)]\n",
    "        avg_conf = float(sum(confs) / max(1, len(confs))) if confs else 0.0\n",
    "    except Exception:\n",
    "        avg_conf = 0.0\n",
    "    return txt, avg_conf\n",
    "\n",
    "def pdf_page_to_image(pdf: fitz.Document, page_index: int, dpi: int = PDF_DPI) -> Image.Image:\n",
    "    page = pdf.load_page(page_index)\n",
    "    mat = fitz.Matrix(dpi/72, dpi/72)\n",
    "    pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    return img\n",
    "\n",
    "def ocr_image_path(path: Path) -> Dict[str, Any]:\n",
    "    img = Image.open(path)\n",
    "    proc = preprocess_image(img)\n",
    "    text, conf = image_to_text(proc)\n",
    "    return {\n",
    "        \"source_path\": str(path),\n",
    "        \"doc_id\": path.stem,\n",
    "        \"page\": None,\n",
    "        \"text\": text,\n",
    "        \"avg_conf\": conf,\n",
    "        \"lang\": OCR_LANG,\n",
    "        \"psm\": OCR_PSM,\n",
    "        \"oem\": OCR_OEM,\n",
    "        \"dpi\": None,\n",
    "        \"kind\": \"image\"\n",
    "    }\n",
    "\n",
    "def ocr_pdf_path(path: Path) -> List[Dict[str, Any]]:\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    with fitz.open(str(path)) as doc:\n",
    "        for pno in tqdm(range(doc.page_count), desc=f\"OCR {path.name}\"):\n",
    "            img = pdf_page_to_image(doc, pno, dpi=PDF_DPI)\n",
    "            proc = preprocess_image(img)\n",
    "            text, conf = image_to_text(proc)\n",
    "            out.append({\n",
    "                \"source_path\": str(path),\n",
    "                \"doc_id\": path.stem,\n",
    "                \"page\": pno + 1,\n",
    "                \"text\": text,\n",
    "                \"avg_conf\": conf,\n",
    "                \"lang\": OCR_LANG,\n",
    "                \"psm\": OCR_PSM,\n",
    "                \"oem\": OCR_OEM,\n",
    "                \"dpi\": PDF_DPI,\n",
    "                \"kind\": \"pdf_page\"\n",
    "            })\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning OCR output\n",
    "- Normalize whitespace and line breaks\n",
    "- Fix common hyphenation across line breaks (word-)\n",
    "- Remove obvious junk lines (very short, all punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ocr_text(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = re.sub(r\"(\\w)-\\s*\\n\\s*(\\w)\", r\"\\1\\2\", s)\n",
    "    s = re.sub(r\"[ \\t]\\n[ \\t]\", \"\\n\", s)\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    lines = s.splitlines()\n",
    "    keep = []\n",
    "    for ln in lines:\n",
    "        ln2 = ln.strip()\n",
    "        if not ln2:\n",
    "            keep.append(\"\")\n",
    "            continue\n",
    "        alnum = sum(ch.isalnum() for ch in ln2)\n",
    "        if alnum < max(3, int(0.2 * len(ln2))):\n",
    "            continue\n",
    "        keep.append(ln2)\n",
    "    s = \"\\n\".join(keep)\n",
    "    s = re.sub(r\"[ \\t]{2,}\", \" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking (simple, overlap-aware)\n",
    "- Split cleaned text to chunks with overlap\n",
    "- Record char spans for traceability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, max_chars: int = MAX_CHARS, overlap: int = OVERLAP) -> List[Dict[str, Any]]:\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        end = min(len(text), i + max_chars)\n",
    "        t = text[i:end]\n",
    "        chunks.append({\"text\": t, \"char_start\": i, \"char_end\": i + len(t)})\n",
    "        if end >= len(text):\n",
    "            break\n",
    "        i = max(0, end - overlap)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OCR runners\n",
    "- Process all images in INPUT_IMG_DIR (jpg/png)\n",
    "- Process all PDFs in INPUT_PDF_DIR\n",
    "- Produce chunk records and write a JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_images_in_dir(img_dir: Path) -> List[Dict[str, Any]]:\n",
    "    records: List[Dict[str, Any]] = []\n",
    "    if not img_dir.exists():\n",
    "        return records\n",
    "    files = [p for p in img_dir.glob(\"*\") if p.suffix.lower() in {\".jpg\", \".jpeg\", \".png\", \".tiff\", \".bmp\"}]\n",
    "    for p in tqdm(files, desc=\"OCR images\"):\n",
    "        rec = ocr_image_path(p)\n",
    "        clean = clean_ocr_text(rec[\"text\"])\n",
    "        chs = chunk_text(clean)\n",
    "        for idx, ch in enumerate(chs):\n",
    "            records.append({\n",
    "                \"id\": f\"{rec['doc_id']}img{idx}\",\n",
    "                \"text\": ch[\"text\"],\n",
    "                \"metadata\": {\n",
    "                    \"doc_id\": rec[\"doc_id\"],\n",
    "                    \"source_path\": rec[\"source_path\"],\n",
    "                    \"page\": None,\n",
    "                    \"char_start\": ch[\"char_start\"],\n",
    "                    \"char_end\": ch[\"char_end\"],\n",
    "                    \"ocr_avg_conf\": rec[\"avg_conf\"],\n",
    "                    \"ocr_lang\": rec[\"lang\"],\n",
    "                    \"ocr_psm\": rec[\"psm\"],\n",
    "                    \"ocr_oem\": rec[\"oem\"],\n",
    "                    \"dpi\": rec[\"dpi\"],\n",
    "                    \"kind\": rec[\"kind\"]\n",
    "                }\n",
    "            })\n",
    "    return records\n",
    "\n",
    "def ocr_pdfs_in_dir(pdf_dir: Path) -> List[Dict[str, Any]]:\n",
    "    records: List[Dict[str, Any]] = []\n",
    "    if not pdf_dir.exists():\n",
    "        return records\n",
    "    files = [p for p in pdf_dir.glob(\"*.pdf\")]\n",
    "    for p in files:\n",
    "        page_recs = ocr_pdf_path(p)\n",
    "        for pr in page_recs:\n",
    "            clean = clean_ocr_text(pr[\"text\"])\n",
    "            chs = chunk_text(clean)\n",
    "            for idx, ch in enumerate(chs):\n",
    "                records.append({\n",
    "                    \"id\": f\"{pr['doc_id']}p{pr['page']}{idx}\",\n",
    "                    \"text\": ch[\"text\"],\n",
    "                    \"metadata\": {\n",
    "                        \"doc_id\": pr[\"doc_id\"],\n",
    "                        \"source_path\": pr[\"source_path\"],\n",
    "                        \"page\": pr[\"page\"],\n",
    "                        \"char_start\": ch[\"char_start\"],\n",
    "                        \"char_end\": ch[\"char_end\"],\n",
    "                        \"ocr_avg_conf\": pr[\"avg_conf\"],\n",
    "                        \"ocr_lang\": pr[\"lang\"],\n",
    "                        \"ocr_psm\": pr[\"psm\"],\n",
    "                        \"ocr_oem\": pr[\"oem\"],\n",
    "                        \"dpi\": pr[\"dpi\"],\n",
    "                        \"kind\": pr[\"kind\"]\n",
    "                    }\n",
    "                })\n",
    "    return records\n",
    "\n",
    "def save_jsonl(records: List[Dict[str, Any]], out_path: Path):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run OCR and write chunks\n",
    "- Produces ocr_chunks.jsonl in OUTPUT_DIR\n",
    "- You can merge these with Module 2 chunks if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OCR images: 100%|██████████| 3/3 [00:06<00:00,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR chunks written: 3 -> ..\\data\\ocr_outputs\\ocr_chunks.jsonl\n",
      "factsimg0 | conf: 94.96551724137932 | page: None\n",
      "indiatoday®  Heart has its own electrical impulse so, it can continue to beat even when separated from the body, as long has it has a supply of oxygen ...\n",
      "\n",
      "spandanimg0 | conf: 84.87755102040816 | page: None\n",
      "4 -spandap’ How to Keep Heart Healthy A Comprehensive Guide for Every Age les! Eat heart-friendly, balanced diet kK Exercise regularly for stronger heart ar Manage stress with mindfulness techniques @ ...\n",
      "\n",
      "spandan_factsimg0 | conf: 93.02272727272727 | page: None\n",
      "O04   15 Facts About the Heart That You Didn't Know  Can beat outside body 08 Heart cells stop dividing early in life Laughter benefits O09 Heart can enlarge due to exercise or disease Represents love ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ocr_records = []\n",
    "ocr_records += ocr_images_in_dir(INPUT_IMG_DIR)\n",
    "ocr_records += ocr_pdfs_in_dir(INPUT_PDF_DIR)\n",
    "out_ocr = OUTPUT_DIR / \"ocr_chunks.jsonl\"\n",
    "save_jsonl(ocr_records, out_ocr)\n",
    "print(f\"OCR chunks written: {len(ocr_records)} -> {out_ocr}\")\n",
    "for r in ocr_records[:3]:\n",
    "    print(r[\"id\"], \"| conf:\", r[\"metadata\"][\"ocr_avg_conf\"], \"| page:\", r[\"metadata\"][\"page\"])\n",
    "    print(r[\"text\"][:200].replace(\"\\n\",\" \"), \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector store integration (FAISS)\n",
    "- Load existing FAISS index from Module 3 (if present) and append OCR chunks, or create a new index.\n",
    "- We store embeddings (L2-normalized), FAISS index, docstore.jsonl, vectors.npy, and manifest.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize(vecs: np.ndarray) -> np.ndarray:\n",
    "    norms = np.linalg.norm(vecs, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1.0\n",
    "    return vecs / norms\n",
    "\n",
    "def build_faiss_index(vectors: np.ndarray) -> faiss.Index:\n",
    "    dim = vectors.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(vectors)\n",
    "    return index\n",
    "\n",
    "def load_index(in_dir: Path) -> Tuple[faiss.Index, np.ndarray, List[Dict[str, Any]], Dict[str, Any]]:\n",
    "    index = faiss.read_index(str(in_dir / \"index.faiss\"))\n",
    "    vectors = np.load(in_dir / \"vectors.npy\")\n",
    "    docstore = []\n",
    "    with (in_dir / \"docstore.jsonl\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                docstore.append(json.loads(line))\n",
    "    manifest = json.loads((in_dir / \"manifest.json\").read_text(encoding=\"utf-8\"))\n",
    "    return index, vectors, docstore, manifest\n",
    "\n",
    "def save_index(index: faiss.Index, vectors: np.ndarray, docstore: List[Dict[str, Any]], backend_name: str, dim: int, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    faiss.write_index(index, str(out_dir / \"index.faiss\"))\n",
    "    np.save(out_dir / \"vectors.npy\", vectors)\n",
    "    with (out_dir / \"docstore.jsonl\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for d in docstore:\n",
    "            f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "    manifest = {\"backend\": backend_name, \"dim\": dim, \"count\": int(vectors.shape[0]), \"created_at\": time.time()}\n",
    "    with (out_dir / \"manifest.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(manifest, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def prepare_texts(records: List[Dict[str, Any]], max_len: int = 4000) -> List[str]:\n",
    "    out = []\n",
    "    for r in records:\n",
    "        t = (r[\"text\"] or \"\").strip()\n",
    "        out.append(t[:max_len] if max_len and len(t) > max_len else t)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemini embedding: 100%|██████████| 1/1 [00:02<00:00,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index updated. Total vectors: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "texts = prepare_texts(ocr_records)\n",
    "if not texts:\n",
    "    print(\"No OCR texts to embed. Skipping index update.\")\n",
    "else:\n",
    "    X = backend.encode(texts, batch_size=64).astype(np.float32)\n",
    "    X = l2_normalize(X)\n",
    "    new_docs = [{\"id\": r[\"id\"], \"text\": r[\"text\"], \"metadata\": r[\"metadata\"]} for r in ocr_records]\n",
    "    index_path = INDEX_DIR / \"index.faiss\"\n",
    "    if index_path.exists():\n",
    "        index, vectors, docstore, manifest = load_index(INDEX_DIR)\n",
    "        if vectors.shape[1] != X.shape[1]:\n",
    "            raise ValueError(f\"Dim mismatch: existing {vectors.shape[1]} vs new {X.shape[1]}. Use the same embedding backend.\")\n",
    "        index.add(X)\n",
    "        vectors = np.concatenate([vectors, X], axis=0)\n",
    "        docstore.extend(new_docs)\n",
    "        save_index(index, vectors, docstore, backend_name=EMBEDDING_BACKEND, dim=vectors.shape[1], out_dir=INDEX_DIR)\n",
    "        print(f\"Index updated. Total vectors: {vectors.shape[0]}\")\n",
    "    else:\n",
    "        index = build_faiss_index(X)\n",
    "        vectors = X\n",
    "        docstore = new_docs\n",
    "        save_index(index, vectors, docstore, backend_name=EMBEDDING_BACKEND, dim=vectors.shape[1], out_dir=INDEX_DIR)\n",
    "        print(f\"New index created at {INDEX_DIR} with {vectors.shape[0]} vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick search to validate OCR content is indexed\n",
    "- Simple cosine search using FAISS IndexFlatIP and normalized vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: electrical impulse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemini embedding: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    indiatoday®  Heart has its own electrical impulse so, it can continue to beat even when separated from the body, as long has it has a supply of oxygen ...\n",
      "    O04   15 Facts About the Heart That You Didn't Know  Can beat outside body 08 Heart cells stop dividing early in life Laughter benefits O09 Heart can enlarge due to exercise or disease Represents love ...\n",
      "    olic (lower number) is the pressure against the walls of the arteries during  the relaxation phase (the heart is at rest between beats).  •  High blood pressure is called the “silent killer” because t ...\n",
      "\n",
      "Query: heart cells\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemini embedding: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    O04   15 Facts About the Heart That You Didn't Know  Can beat outside body 08 Heart cells stop dividing early in life Laughter benefits O09 Heart can enlarge due to exercise or disease Represents love ...\n",
      "    indiatoday®  Heart has its own electrical impulse so, it can continue to beat even when separated from the body, as long has it has a supply of oxygen ...\n",
      "    Heart Health  In the United States, heart disease is the leading cause of death for both men and women. People with  pre-diabetes and/or the metabolic syndrome are at higher risk for developing heart  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def embed_query(q: str, backend) -> np.ndarray:\n",
    "    v = backend.encode([q], batch_size=1).astype(np.float32)\n",
    "    return l2_normalize(v)\n",
    "\n",
    "def search(index: faiss.Index, vectors: np.ndarray, docstore: List[Dict[str, Any]], query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    qv = embed_query(query, backend)\n",
    "    D, I = index.search(qv, top_k)\n",
    "    res = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        d = docstore[idx]\n",
    "        res.append({\"score\": float(score), \"id\": d[\"id\"], \"text\": d[\"text\"], \"metadata\": d[\"metadata\"]})\n",
    "    return res\n",
    "\n",
    "if (INDEX_DIR / \"index.faiss\").exists():\n",
    "    idx, vecs, store, mani = load_index(INDEX_DIR)\n",
    "    for q in [\"electrical impulse\", \"heart cells\"]:\n",
    "        print(f\"\\nQuery: {q}\")\n",
    "        results = search(idx, vecs, store, q, top_k=3)\n",
    "        for r in results:\n",
    "            m = r[\"metadata\"]\n",
    "            loc = f\"{m.get('doc_id')} p.{m.get('page')}\" if m.get(\"page\") else m.get(\"doc_id\")\n",
    "            # print(f\"  score={r['score']:.3f} | {loc} | kind={m.get('kind')} | conf={m.get('ocr_avg_conf'):.1f}\")\n",
    "            print(\"   \", r[\"text\"][:200].replace(\"\\n\",\" \"), \"...\")\n",
    "else:\n",
    "    print(\"Index not found; skipped query demo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini Task\n",
    "1) Put a scanned PDF in data/scanned_pdfs or images in data/images.\n",
    "2) Run OCR cells to produce ocr_chunks.jsonl.\n",
    "3) Run embedding + FAISS update cells to add OCR chunks to your existing index.\n",
    "4) Run the quick search to verify OCR text is retrievable (with metadata like page and avg confidence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes and troubleshooting\n",
    "- Multi-language OCR: install language packs (e.g., tesseract-ocr-deu) and set OCR_LANG=\"eng+deu\".\n",
    "- PSM tuning: PSM 6 or 4 can help for single-column docs; 11/13 for sparse text.\n",
    "- DPI: 300 improves accuracy for low-res scans; higher DPI increases time and memory.\n",
    "- Deskew: OSD may fail on very noisy pages; consider OpenCV-based deskew for robust rotation correction.\n",
    "- Junk removal: Adjust cleaning heuristics if you see important lines removed.\n",
    "- Confidence: Average conf is approximate; use to flag low-quality OCR for reprocessing.\n",
    "- Index mixing: Keep the same embedding backend/dimension across updates (don’t mix MiniLM and Gemini in one index).\n",
    "- Pipeline: Load → Preprocess → OCR → Clean → Chunk → Embed → Index. This clean separation follows the MCP-style flow and makes it easy to swap steps later.\n",
    "This sets you up for Module 5 (RAG pipeline) where we’ll wire retrieval to an LLM with prompts that cite page numbers and sources, and Module 6 (agent memory + multi-step reasoning)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DocumentAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
