{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 5 — Building a RAG Pipeline (LangChain)\n",
    "Objectives\n",
    "- User Query → Embed → Search vector DB → Feed to LLM\n",
    "- Prompt templates for concise, source-backed answers\n",
    "- Two approaches:\n",
    "1) Custom LCEL RAG chain\n",
    "2) LangChain RetrievalQA chain\n",
    "- Include citations with metadata (e.g., [doc_id p.43])\n",
    "Prereqs\n",
    "- You built an index in Module 3 at ./data/indexes/my_corpus\n",
    "- Set an LLM key (prefer Gemini):\n",
    "- export GOOGLE_API_KEY=...\n",
    "- Optional fallback: OpenAI (export OPENAI_API_KEY=...), but this notebook defaults to Gemini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies (if needed)\n",
    "- langchain, core, community, google-genai client\n",
    "- sentence-transformers (for MiniLM query embeddings)\n",
    "- faiss-cpu\n",
    "- tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Python: 3.12.9 | FAISS ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain langchain-core langchain-community langchain-google-genai google-generativeai\n",
    "%pip install -q sentence-transformers faiss-cpu tqdm\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# FAISS import\n",
    "try:\n",
    "    import faiss  # noqa: F401\n",
    "except Exception:\n",
    "    import faiss_cpu as faiss  # type: ignore\n",
    "print(f\"Python: {sys.version.split()[0]} | FAISS ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the saved FAISS index and docstore (from Module 3)\n",
    "- We rely on the manifest to detect which embedding backend was used.\n",
    "- Important: Query embeddings MUST use the same backend/dimension as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index: dim=768, vectors=19, backend=gemini\n"
     ]
    }
   ],
   "source": [
    "INDEX_DIR = Path(\"./../data/indexes/my_corpus\")\n",
    "assert (INDEX_DIR / \"index.faiss\").exists(), f\"FAISS index not found at {INDEX_DIR}. Please run Module 3.\"\n",
    "\n",
    "def load_index(in_dir: Path) -> Tuple[faiss.Index, np.ndarray, List[Dict[str, Any]], Dict[str, Any]]:\n",
    "    index = faiss.read_index(str(in_dir / \"index.faiss\"))\n",
    "    vectors = np.load(in_dir / \"vectors.npy\")\n",
    "    docstore = []\n",
    "    with (in_dir / \"docstore.jsonl\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                docstore.append(json.loads(line))\n",
    "    manifest = json.loads((in_dir / \"manifest.json\").read_text(encoding=\"utf-8\"))\n",
    "    return index, vectors, docstore, manifest\n",
    "\n",
    "index, vectors, store, manifest = load_index(INDEX_DIR)\n",
    "print(f\"Loaded index: dim={vectors.shape[1]}, vectors={vectors.shape[0]}, backend={manifest.get('backend')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding backend for queries\n",
    "- MiniLM (local, free) or Gemini (cloud).\n",
    "- Must match the backend used when the index was built (see manifest.json).\n",
    "- If manifest says 'gemini', you need GOOGLE_API_KEY set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend from manifest: gemini\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(r\"C:\\ML\\LU-LiveClasses\\DocumentAI\\.env\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"\")\n",
    "BACKEND = manifest.get(\"backend\", \"minilm\").lower()\n",
    "print(\"Backend from manifest:\", BACKEND)\n",
    "\n",
    "class MiniLMBackend:\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        import torch\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "        self.dim = self.model.get_sentence_embedding_dimension()\n",
    "        self.model_name = model_name\n",
    "    def encode(self, texts: List[str], batch_size: int = 64) -> np.ndarray:\n",
    "        return self.model.encode(texts, batch_size=batch_size, convert_to_numpy=True, normalize_embeddings=False).astype(np.float32)\n",
    "\n",
    "class GeminiBackend:\n",
    "    def __init__(self, model_name: str = \"text-embedding-004\", api_key: Optional[str] = None):\n",
    "        assert api_key, \"GOOGLE_API_KEY required for Gemini embeddings.\"\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.genai = genai\n",
    "        self.model_name = model_name\n",
    "        self.dim = 768\n",
    "    def encode(self, texts: List[str], batch_size: int = 16) -> np.ndarray:\n",
    "        vecs: List[List[float]] = []\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Gemini embed\"):\n",
    "            for t in texts[i:i+batch_size]:\n",
    "                resp = self.genai.embed_content(model=self.model_name, content=t[:4000])\n",
    "                vecs.append(resp[\"embedding\"])\n",
    "        return np.asarray(vecs, dtype=np.float32)\n",
    "\n",
    "def l2_normalize(vecs: np.ndarray) -> np.ndarray:\n",
    "    norms = np.linalg.norm(vecs, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1.0\n",
    "    return vecs / norms\n",
    "\n",
    "if BACKEND == \"minilm\":\n",
    "    query_backend = MiniLMBackend()\n",
    "elif BACKEND == \"gemini\":\n",
    "    query_backend = GeminiBackend(api_key=GOOGLE_API_KEY)\n",
    "else:\n",
    "    raise ValueError(\"Unknown backend in manifest. Expected 'minilm' or 'gemini'.\")\n",
    "\n",
    "assert query_backend.dim == vectors.shape[1], f\"Dim mismatch: backend {query_backend.dim} vs index {vectors.shape[1]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a LangChain retriever around the existing FAISS index\n",
    "- We wrap our FAISS + vectors + docstore in a custom BaseRetriever.\n",
    "- Returns LangChain Documents with page_content and metadata for citations.\n",
    "- Optional MMR re-ranking for diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from pydantic import BaseModel, Field\n",
    "import numpy as np\n",
    "\n",
    "def embed_query(q: str) -> np.ndarray:\n",
    "    v = query_backend.encode([q], batch_size=1)\n",
    "    return l2_normalize(v)\n",
    "\n",
    "def search_faiss(index: faiss.Index, qvec: np.ndarray, top_k: int = 5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    D, I = index.search(qvec, top_k)\n",
    "    return D[0], I[0]\n",
    "\n",
    "def mmr_rerank(query_vec: np.ndarray,\n",
    "               candidates_idx: np.ndarray,\n",
    "               candidate_vecs: np.ndarray,\n",
    "               top_k: int = 5,\n",
    "               lambda_mult: float = 0.5) -> List[int]:\n",
    "    selected: List[int] = []\n",
    "    pool = candidates_idx.tolist()\n",
    "    sims_q = (candidate_vecs @ query_vec[0])\n",
    "    while len(selected) < min(top_k, len(pool)):\n",
    "        best_i = None\n",
    "        best_score = -1e9\n",
    "        for idx in pool:\n",
    "            rel = sims_q[idx]\n",
    "            if not selected:\n",
    "                score = rel\n",
    "            else:\n",
    "                div = max(candidate_vecs[idx] @ candidate_vecs[j] for j in selected)\n",
    "                score = lambda_mult * rel - (1 - lambda_mult) * div\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_i = idx\n",
    "        selected.append(best_i)\n",
    "        pool.remove(best_i)\n",
    "    return selected\n",
    "\n",
    "class FaissJSONLRetriever(BaseRetriever, BaseModel):\n",
    "    index: Any\n",
    "    vectors: np.ndarray\n",
    "    store: List[Dict[str, Any]]\n",
    "    top_k: int = 5\n",
    "    use_mmr: bool = False\n",
    "    mmr_lambda: float = 0.5\n",
    "    fetch_k: int = 25  # number to fetch before MMR\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        qv = embed_query(query)\n",
    "        top_k = self.top_k\n",
    "        if self.use_mmr:\n",
    "            D, I = search_faiss(self.index, qv, self.fetch_k)\n",
    "            selected = mmr_rerank(qv, I, self.vectors, top_k=top_k, lambda_mult=self.mmr_lambda)\n",
    "            I = np.array(selected, dtype=int)\n",
    "        else:\n",
    "            D, I = search_faiss(self.index, qv, top_k)\n",
    "        docs: List[Document] = []\n",
    "        for idx in I[:top_k]:\n",
    "            rec = self.store[idx]\n",
    "            docs.append(Document(page_content=rec[\"text\"], metadata=rec.get(\"metadata\", {})))\n",
    "        return docs\n",
    "\n",
    "retriever = FaissJSONLRetriever(index=index, vectors=vectors, store=store, top_k=5, use_mmr=True, mmr_lambda=0.5, fetch_k=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM setup (Gemini preferred)\n",
    "- Uses ChatGoogleGenerativeAI (gemini-1.5-flash) for fast, cost-effective answers.\n",
    "- If GOOGLE_API_KEY is missing, you can switch to ChatOpenAI by setting OPENAI_API_KEY and adjusting the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Gemini for answers.\n"
     ]
    }
   ],
   "source": [
    "USE_GEMINI = bool(GOOGLE_API_KEY)\n",
    "\n",
    "if USE_GEMINI:\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.2, max_output_tokens=512)\n",
    "    print(\"Using Gemini for answers.\")\n",
    "else:\n",
    "    # Optional fallback to OpenAI if desired:\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "    if OPENAI_API_KEY:\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "        print(\"Using OpenAI fallback. Set GOOGLE_API_KEY to use Gemini.\")\n",
    "    else:\n",
    "        raise EnvironmentError(\"No LLM configured. Set GOOGLE_API_KEY (preferred) or OPENAI_API_KEY.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 1: Custom LCEL RAG chain\n",
    "- Format retrieved documents into a compact context with citations.\n",
    "- Prompt instructs the model to cite sources like [doc_id p.page].\n",
    "- Returns a concise, source-backed answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    lines = []\n",
    "    for d in docs:\n",
    "        m = d.metadata\n",
    "        cite = f\"{m.get('doc_id','?')} p.{m.get('page','?')}\"\n",
    "        lines.append(f\"[{cite}] {d.page_content}\")\n",
    "    return \"\\n\\n\".join(lines)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a concise, domain-agnostic assistant. Use ONLY the provided context to answer.\\n\"\n",
    "     \"Always cite sources in brackets like [doc_id p.page]. If unsure, say you don't know.\\n\"\n",
    "     \"Be precise and avoid speculation.\"),\n",
    "    (\"human\",\n",
    "     \"Question: {question}\\n\\nContext:\\n{context}\\n\\nAnswer:\")\n",
    "])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | RunnableLambda(format_docs), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the custom RAG chain\n",
    "- Try queries relevant to your corpus (e.g., contracts: termination, confidentiality).\n",
    "- The answer should include bracketed citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: Can heart beat outside body?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemini embed: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, a heart can beat outside the body as long as it receives oxygen [facts p.None].\n",
      "\n",
      "Q: what is full form of HDL?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemini embed: 100%|██████████| 1/1 [00:00<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDL stands for high-density lipoprotein [heart-health p.1].\n",
      "\n",
      "Q: Impact of lifestyle and weight on heart?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemini embed: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obesity and being overweight are risk factors for heart disease [heart-health p.1].  Maintaining a healthy weight can reduce heart disease risk [heart-health p.1, How-to-Monitor-Cholesterol-BP-Weight p.2].  Lifestyle changes, including increased physical activity (at least 150 minutes of moderate-intensity aerobic activity per week) and reducing caloric intake, can help with weight loss [How-to-Monitor-Cholesterol-BP-Weight p.2].  Even a 3-5% weight loss can offer health benefits, with greater benefits seen with larger weight losses (5-10%) [How-to-Monitor-Cholesterol-BP-Weight p.2].\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"Can heart beat outside body?\",\n",
    "    \"what is full form of HDL?\",\n",
    "    \"Impact of lifestyle and weight on heart?\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    ans = rag_chain.invoke(q)\n",
    "    print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 2: LangChain RetrievalQA chain\n",
    "- Uses the same retriever, but leverages the built-in RetrievalQA helper.\n",
    "- We pass a custom prompt to enforce citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: How to Lower cholestrol?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemini embed: 100%|██████████| 1/1 [00:00<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To lower cholesterol, eat a diet rich in whole grains, plant-based proteins, fish, nontropical vegetable oils, nuts, and seeds [How-to-Monitor-Cholesterol-BP-Weight p.2].  Limit sodium, sweets, sugar-sweetened beverages, processed meats, trans fats, and refined carbohydrates [How-to-Monitor-Cholesterol-BP-Weight p.2]. Eat at least two servings of fatty fish per week [How-to-Monitor-Cholesterol-BP-Weight p.2]. Consume less than 1,500 mg of sodium daily [How-to-Monitor-Cholesterol-BP-Weight p.2]. Limit alcohol consumption [How-to-Monitor-Cholesterol-BP-Weight p.2].  Be physically active [How-to-Monitor-Cholesterol-BP-Weight p.2]. Maintain a healthy weight [How-to-Monitor-Cholesterol-BP-Weight p.2], don't smoke, and take prescribed medications [How-to-Monitor-Cholesterol-BP-Weight p.2].\n",
      "\n",
      "Q: What is Spandan?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemini embed: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text, Spandan is a portable ECG. [spandan p.None]\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a concise assistant. Use ONLY the retrieved context to answer.\\n\"\n",
    "     \"Cite sources in brackets like [doc_id p.page]. If the answer is not in the context, say you don't know.\"),\n",
    "    (\"human\",\n",
    "     \"Question: {question}\\n\\nRetrieved context:\\n{context}\\n\\nAnswer:\")\n",
    "])\n",
    "\n",
    "# The RetrievalQA 'stuff' chain passes a {context} string of concatenated docs.\n",
    "# We'll enforce our own formatting by wrapping the retriever with a small adapter.\n",
    "def retriever_to_context(query: str) -> str:\n",
    "    # LCEL adapter to get docs and format them\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    return format_docs(docs)\n",
    "\n",
    "retrievalqa_chain = (\n",
    "    {\"context\": RunnableLambda(retriever_to_context), \"question\": RunnablePassthrough()}\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for q in [\"How to Lower cholestrol?\", \"What is Spandan?\"]:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    ans = retrievalqa_chain.invoke(q)\n",
    "    print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-processing: Extract and display cited sources explicitly\n",
    "- Pull out top-k docs used to build the answer so you can show clickable citations in a UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemini embed: 100%|██████████| 1/1 [00:00<00:00,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sources used:\n",
      "- heart-health p.1 | section=None | ..\\data\\pdfs\\heart-health.pdf\n",
      "- spandan p.None | section=None | ..\\data\\images\\spandan.png\n",
      "- spandan_facts p.None | section=None | ..\\data\\images\\spandan_facts.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_sources_for_query(query: str, k: int = 5) -> List[Dict[str, Any]]:\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    out = []\n",
    "    for d in docs[:k]:\n",
    "        m = d.metadata\n",
    "        out.append({\n",
    "            \"doc_id\": m.get(\"doc_id\"),\n",
    "            \"page\": m.get(\"page\"),\n",
    "            \"section_title\": m.get(\"section_title\"),\n",
    "            \"source_path\": m.get(\"source_path\")\n",
    "        })\n",
    "    return out\n",
    "\n",
    "q = \"how to prevent heart disease?\"\n",
    "sources = get_sources_for_query(q, k=3)\n",
    "print(\"\\nSources used:\")\n",
    "for s in sources:\n",
    "    print(f\"- {s['doc_id']} p.{s['page']} | section={s['section_title']} | {s['source_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini Task\n",
    "1) Ensure your FAISS index from Module 3 exists at ./data/indexes/my_corpus.\n",
    "2) Set GOOGLE_API_KEY (preferred) or OPENAI_API_KEY.\n",
    "3) Run:\n",
    "- Build retriever\n",
    "- Custom LCEL RAG chain tests\n",
    "- RetrievalQA chain tests\n",
    "4) Verify answers are concise and include citations like [doc_id p.page]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips and notes\n",
    "- Prompting: Keep the instruction explicit about using only provided context and citing sources.\n",
    "- Chunking overlap (Module 2) helps provide enough context per chunk to answer questions cleanly.\n",
    "- Top-k tuning: Start with k=4–6; increase if answers frequently say “don’t know.”\n",
    "- MMR: Improves diversity when many near-duplicate chunks exist. Trade-off with recall.\n",
    "- Hallucinations: Use low temperature and strong instructions; consider adding “If any part is uncertain, state the uncertainty.”\n",
    "- Token control: If contexts are large, truncate each chunk to a max length or use a reranker to keep only the most relevant passages.\n",
    "- Mixing sources: If you added OCR chunks in Module 4, they’re seamlessly retrievable with the same retriever/index (as long as the embedding backend matches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps (Module 6)\n",
    "- Add conversational memory with LangChain’s ConversationBufferMemory.\n",
    "- Persist per-session history and support follow-up questions that reference prior answers.\n",
    "- Introduce tool-using agent patterns for multi-step reasoning (e.g., retrieval + calculator + web search)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DocumentAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
