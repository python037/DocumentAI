{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d49803d",
   "metadata": {},
   "source": [
    "# Module 2 — Parsing Large PDFs Efficiently\n",
    "\n",
    "## Objectives\n",
    "- Extract text from large PDFs using **PyMuPDF** efficiently. 📄\n",
    "- Implement **smart chunking** with overlap and hierarchical hints (TOC, headings).\n",
    "- Support multiple PDFs in a directory.\n",
    "- Store **rich metadata** (doc_id, page number, section title/path, char offsets).\n",
    "- **Mini Task**: Upload a long PDF → Chunk → Save with metadata. ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13ec9f5",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "- **PyMuPDF (fitz)**: PDF parsing and layout info.\n",
    "- **tqdm**: progress bars.\n",
    "- **Optional** (not required): `pytesseract` + `PIL` for OCR fallback when scanned pages are detected. We add a stub that only runs if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8fd9790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m ensurepip --upgrade\n",
    "# !{sys.executable} -m pip install PyMuPDF\n",
    "# !{sys.executable} -m pip install tqdm\n",
    "# !{sys.executable} -m pip install -q pillow pytesseract\n",
    "# !{sys.executable} -m pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d400f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.9, PyMuPDF: 1.26.3\n",
      "OCR available: True\n"
     ]
    }
   ],
   "source": [
    "# If running in a fresh environment, uncomment:\n",
    "# %pip install -q pymupdf tqdm\n",
    "\n",
    "# Optional OCR (only if you plan to use it)\n",
    "# %pip install -q pillow pytesseract\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from tqdm.notebook import tqdm # Use notebook-friendly version of tqdm\n",
    "\n",
    "try:\n",
    "    from PIL import Image\n",
    "    import pytesseract\n",
    "    OCR_AVAILABLE = True\n",
    "except Exception:\n",
    "    OCR_AVAILABLE = False\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}, PyMuPDF: {fitz.version[0]}\")\n",
    "print(f\"OCR available: {OCR_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1794bde",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "- `INPUT_DIR`: directory of PDFs or a single PDF path.\n",
    "- `OUTPUT_DIR`: where chunk JSONL files will be written.\n",
    "- `Chunk params`: max_chars and overlap.\n",
    "- `Heuristics`: minimum repeated header/footer frequency to strip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07d8a912",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = Path(\"./data/pdfs\")  # set to a folder or a single file path\n",
    "OUTPUT_DIR = Path(\"./data/outputs\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Chunking parameters\n",
    "MAX_CHARS = 1200\n",
    "OVERLAP = 200\n",
    "\n",
    "# Header/Footer detection parameters\n",
    "HEADER_FOOTER_MIN_FREQ = 0.2  # if a line repeats on >=20% of pages, treat as header/footer\n",
    "MIN_HEADER_FOOTER_LEN = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a3f09f",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "- Safe JSONL writer\n",
    "- Token estimate (optional, for info)\n",
    "- Hash helper for deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl(records: List[Dict[str, Any]], out_path: Path):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def load_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    items = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                items.append(json.loads(line))\n",
    "    return items\n",
    "\n",
    "def est_tokens_from_chars(n_chars: int) -> int:\n",
    "    return max(1, n_chars // 4)  # crude approximation\n",
    "\n",
    "def text_hash(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()[:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef3316e",
   "metadata": {},
   "source": [
    "## Building a Table of Contents (TOC) map\n",
    "- PyMuPDF `get_toc(simple=True)` returns `[level, title, page_num(1-based)]`\n",
    "- We convert it to a page-indexed map of the nearest section path for each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "138e9194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_toc_map(doc: fitz.Document) -> Dict[int, Dict[str, Any]]:\n",
    "    toc = doc.get_toc(simple=True) or []\n",
    "    # Normalize to 0-based pages\n",
    "    norm = [(lvl, title.strip(), max(0, page - 1)) for (lvl, title, page) in toc]\n",
    "    \n",
    "    # Build a running section path per page\n",
    "    page_to_section = {}\n",
    "    current_path: List[str] = []\n",
    "    current_page = 0\n",
    "    i = 0\n",
    "    \n",
    "    while current_page < doc.page_count:\n",
    "        # Advance TOC entries that start on or before current_page\n",
    "        while i < len(norm) and norm[i][2] <= current_page:\n",
    "            lvl, title, _ = norm[i]\n",
    "            # Adjust path to the current level\n",
    "            current_path = current_path[:max(0, lvl - 1)]\n",
    "            current_path.append(title)\n",
    "            i += 1\n",
    "        \n",
    "        page_to_section[current_page] = {\n",
    "            \"section_path\": current_path.copy(),\n",
    "            \"section_title\": current_path[-1] if current_path else None\n",
    "        }\n",
    "        current_page += 1\n",
    "        \n",
    "    return page_to_section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb9773",
   "metadata": {},
   "source": [
    "## Extracting page text\n",
    "- We use `page.get_text(\"text\")` for performance and reliability on digital PDFs.\n",
    "- We detect scanned pages if no text is returned and images exist on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6b8321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page_text(page: fitz.Page) -> Tuple[str, bool]:\n",
    "    txt = page.get_text(\"text\") or \"\"\n",
    "    scanned = False\n",
    "    if not txt.strip():\n",
    "        # If no text but images present, likely scanned\n",
    "        try:\n",
    "            scanned = len(page.get_images(full=True)) > 0\n",
    "        except Exception:\n",
    "            scanned = False\n",
    "    return txt, scanned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04506da",
   "metadata": {},
   "source": [
    "## Optional OCR Fallback\n",
    "- If a page is scanned and OCR is available, we can rasterize the page and run Tesseract.\n",
    "- This is optional and can be slow; only used when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e6c09de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_page(page: fitz.Page, dpi: int = 200) -> str:\n",
    "    if not OCR_AVAILABLE:\n",
    "        return \"\"\n",
    "    mat = fitz.Matrix(dpi / 72, dpi / 72)\n",
    "    pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    return pytesseract.image_to_string(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7318026a",
   "metadata": {},
   "source": [
    "## Header/Footer detection and removal\n",
    "- Many long PDFs repeat headers/footers; we can remove them to improve chunk quality.\n",
    "- **Heuristic**: collect first and last non-empty line of each page; if a line repeats on >= threshold of pages and has sufficient length, treat it as a header/footer and strip it from all pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70ce8efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def first_last_lines_per_page(pages_text: List[str]) -> Tuple[Counter, Counter]:\n",
    "    firsts, lasts = Counter(), Counter()\n",
    "    for t in pages_text:\n",
    "        lines = [ln.strip() for ln in t.splitlines() if ln.strip()]\n",
    "        if lines:\n",
    "            firsts[lines[0]] += 1\n",
    "            lasts[lines[-1]] += 1\n",
    "    return firsts, lasts\n",
    "\n",
    "def detect_repeated_headers_footers(pages_text: List[str], min_freq: float) -> Tuple[set, set]:\n",
    "    n_pages = max(1, len(pages_text))\n",
    "    firsts, lasts = first_last_lines_per_page(pages_text)\n",
    "    \n",
    "    headers = {ln for ln, c in firsts.items() \n",
    "               if c / n_pages >= min_freq and len(ln) >= MIN_HEADER_FOOTER_LEN}\n",
    "    footers = {ln for ln, c in lasts.items() \n",
    "               if c / n_pages >= min_freq and len(ln) >= MIN_HEADER_FOOTER_LEN}\n",
    "    \n",
    "    return headers, footers\n",
    "\n",
    "def strip_headers_footers(text: str, headers: set, footers: set) -> str:\n",
    "    lines = text.splitlines()\n",
    "    # Strip only if exact match (conservative)\n",
    "    if lines and lines[0].strip() in headers:\n",
    "        lines = lines[1:]\n",
    "    if lines and lines[-1].strip() in footers:\n",
    "        lines = lines[:-1]\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45fdf89",
   "metadata": {},
   "source": [
    "## Hierarchical, overlap-aware chunking\n",
    "- Try to split by **paragraph** first, then **sentences**, then hard-wrap by characters with overlap.\n",
    "- Maintain `char_start` and `char_end` offsets within the page text to keep traceability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b41db134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "SENTENCE_SPLIT = re.compile(r\"(?<=[.!?])\\s+\")\n",
    "PARA_SPLIT = re.compile(r\"\\n\\s*\\n+\")\n",
    "\n",
    "def split_to_paragraphs(text: str) -> List[Tuple[int, int, str]]:\n",
    "    spans = []\n",
    "    start = 0\n",
    "    for m in PARA_SPLIT.finditer(text):\n",
    "        end = m.start()\n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk:\n",
    "            spans.append((start, end, chunk))\n",
    "        start = m.end()\n",
    "    # Remainder\n",
    "    if start < len(text):\n",
    "        chunk = text[start:].strip()\n",
    "        if chunk:\n",
    "            spans.append((start, len(text), chunk))\n",
    "    return spans\n",
    "\n",
    "def chunk_hierarchical(text: str, max_chars: int = 1200, overlap: int = 200) -> List[Dict[str, Any]]:\n",
    "    if not text.strip():\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    paras = split_to_paragraphs(text)\n",
    "    buf = \"\"\n",
    "    buf_start, buf_end = 0, 0\n",
    "\n",
    "    def flush_buffer():\n",
    "        nonlocal buf, buf_start, buf_end\n",
    "        if buf:\n",
    "            chunks.append({\"text\": buf, \"char_start\": buf_start, \"char_end\": buf_end})\n",
    "            # Create overlap for the next buffer\n",
    "            if overlap > 0:\n",
    "                overlap_text = buf[-overlap:]\n",
    "                buf = overlap_text\n",
    "                buf_start = buf_end - len(overlap_text)\n",
    "            else:\n",
    "                buf = \"\"\n",
    "                buf_start = buf_end\n",
    "    \n",
    "    for p_start, p_end, p_text in paras:\n",
    "        if not buf:\n",
    "            buf = p_text\n",
    "            buf_start, buf_end = p_start, p_end\n",
    "        elif len(buf) + len(p_text) + 2 <= max_chars:\n",
    "            buf += \"\\n\\n\" + p_text\n",
    "            buf_end = p_end\n",
    "        else:\n",
    "            flush_buffer()\n",
    "            # If the new paragraph is still too large with the overlap, just use the new para\n",
    "            if len(buf) + len(p_text) + 2 > max_chars:\n",
    "                buf = p_text\n",
    "                buf_start, buf_end = p_start, p_end\n",
    "            else:\n",
    "                buf += \"\\n\\n\" + p_text\n",
    "                buf_end = p_end\n",
    "\n",
    "    if buf:  # Flush any remaining buffer\n",
    "        chunks.append({\"text\": buf, \"char_start\": buf_start, \"char_end\": buf_end})\n",
    "        \n",
    "    # Final safeguard for chunks that are still too long (e.g., one very long paragraph)\n",
    "    final_chunks = []\n",
    "    for ch in chunks:\n",
    "        if len(ch[\"text\"]) <= max_chars:\n",
    "            final_chunks.append(ch)\n",
    "            continue\n",
    "        \n",
    "        # Hard split the oversized chunk\n",
    "        text_part = ch[\"text\"]\n",
    "        start_offset = ch[\"char_start\"]\n",
    "        i = 0\n",
    "        while i < len(text_part):\n",
    "            end = i + max_chars\n",
    "            chunk_text = text_part[i:end]\n",
    "            final_chunks.append({\n",
    "                \"text\": chunk_text,\n",
    "                \"char_start\": start_offset + i,\n",
    "                \"char_end\": start_offset + i + len(chunk_text)\n",
    "            })\n",
    "            i += (max_chars - overlap)\n",
    "            \n",
    "    return final_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7191d7",
   "metadata": {},
   "source": [
    "## End-to-end PDF ingestion and chunking for one file\n",
    "- Extract per-page text and scanned flag.\n",
    "- Detect repeated headers/footers and strip.\n",
    "- Use TOC to assign `section_path`/`section_title`; fallback to `None` if no TOC.\n",
    "- Produce chunk records with metadata: `doc_id`, `source_path`, `page`, `section_title/path`, `char_start/end`, `chunk_index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72a506d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PDFIngestStats:\n",
    "    pages_total: int = 0\n",
    "    pages_scanned: int = 0\n",
    "    chunks_total: int = 0\n",
    "    avg_chars_per_chunk: float = 0.0\n",
    "    duration_s: float = 0.0\n",
    "\n",
    "def ingest_pdf(path: Path,\n",
    "             doc_id: Optional[str] = None,\n",
    "             max_chars: int = MAX_CHARS,\n",
    "             overlap: int = OVERLAP,\n",
    "             ocr_scanned: bool = False) -> Tuple[List[Dict[str, Any]], PDFIngestStats]:\n",
    "    t0 = time.time()\n",
    "    path = Path(path)\n",
    "    doc_id = doc_id or path.stem\n",
    "    chunks_out: List[Dict[str, Any]] = []\n",
    "    \n",
    "    with fitz.open(str(path)) as doc:\n",
    "        toc_map = build_toc_map(doc)\n",
    "        pages_text = []\n",
    "        scanned_flags = []\n",
    "        for pno in tqdm(range(doc.page_count), desc=f\"Extracting {path.name}\", leave=False):\n",
    "            page = doc.load_page(pno)\n",
    "            txt, scanned = extract_page_text(page)\n",
    "            if scanned and ocr_scanned and OCR_AVAILABLE:\n",
    "                ocr_txt = ocr_page(page)\n",
    "                if ocr_txt.strip():\n",
    "                    txt = ocr_txt\n",
    "                    scanned = False  # recovered by OCR\n",
    "            pages_text.append(txt)\n",
    "            scanned_flags.append(scanned)\n",
    "\n",
    "        headers, footers = detect_repeated_headers_footers(pages_text, HEADER_FOOTER_MIN_FREQ)\n",
    "\n",
    "        chunk_idx = 0\n",
    "        for pno, (txt, scanned) in enumerate(zip(pages_text, scanned_flags)):\n",
    "            clean_txt = strip_headers_footers(txt, headers, footers)\n",
    "            page_chunks = chunk_hierarchical(clean_txt, max_chars=max_chars, overlap=overlap)\n",
    "            section_meta = toc_map.get(pno, {\"section_title\": None, \"section_path\": []})\n",
    "\n",
    "            for ch in page_chunks:\n",
    "                record = {\n",
    "                    \"id\": f\"{doc_id}_p{pno+1}_{chunk_idx}\",\n",
    "                    \"text\": ch[\"text\"],\n",
    "                    \"metadata\": {\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"source_path\": str(path),\n",
    "                        \"page\": pno + 1,  # human-friendly\n",
    "                        \"char_start\": ch[\"char_start\"],\n",
    "                        \"char_end\": ch[\"char_end\"],\n",
    "                        \"section_title\": section_meta.get(\"section_title\"),\n",
    "                        \"section_path\": section_meta.get(\"section_path\") or [],\n",
    "                        \"scanned\": scanned,\n",
    "                    }\n",
    "                }\n",
    "                chunks_out.append(record)\n",
    "                chunk_idx += 1\n",
    "\n",
    "    dur = time.time() - t0\n",
    "    n_chars = sum(len(c[\"text\"]) for c in chunks_out) or 1\n",
    "    stats = PDFIngestStats(\n",
    "        pages_total=len(pages_text),\n",
    "        pages_scanned=sum(1 for s in scanned_flags if s),\n",
    "        chunks_total=len(chunks_out),\n",
    "        avg_chars_per_chunk=n_chars / max(1, len(chunks_out)),\n",
    "        duration_s=round(dur, 2),\n",
    "    )\n",
    "    return chunks_out, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278f13d9",
   "metadata": {},
   "source": [
    "## Multi-PDF ingestion\n",
    "- Process all PDFs in a directory (non-recursive by default).\n",
    "- Writes one combined JSONL and one per-document JSONL (optional).\n",
    "- Returns combined chunk list and stats summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e977a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_directory(input_path: Path,\n",
    "                     pattern: str = \"*.pdf\",\n",
    "                     per_doc_output: bool = False,\n",
    "                     **ingest_kwargs) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
    "    input_path = Path(input_path)\n",
    "    files = []\n",
    "    if input_path.is_file() and input_path.suffix.lower() == \".pdf\":\n",
    "        files = [input_path]\n",
    "    elif input_path.is_dir():\n",
    "        files = [Path(p) for p in glob.glob(str(input_path / pattern))]\n",
    "    \n",
    "    all_chunks: List[Dict[str, Any]] = []\n",
    "    summary = {\"docs\": []}\n",
    "\n",
    "    for pdf_path in tqdm(files, desc=\"Processing documents\"):\n",
    "        chunks, stats = ingest_pdf(pdf_path, **ingest_kwargs)\n",
    "        all_chunks.extend(chunks)\n",
    "        summary[\"docs\"].append({\n",
    "            \"doc_id\": pdf_path.stem,\n",
    "            \"source_path\": str(pdf_path),\n",
    "            \"stats\": stats.__dict__\n",
    "        })\n",
    "        if per_doc_output:\n",
    "            out_path = OUTPUT_DIR / f\"{pdf_path.stem}_chunks.jsonl\"\n",
    "            save_jsonl(chunks, out_path)\n",
    "\n",
    "    summary[\"total_chunks\"] = len(all_chunks)\n",
    "    summary[\"total_docs\"] = len(files)\n",
    "    return all_chunks, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d05e06",
   "metadata": {},
   "source": [
    "## Mini Task\n",
    "1.  Place a long PDF (100+ pages recommended) in the `data/pdfs` directory or set a direct path.\n",
    "2.  Run ingestion → chunk → save chunks with metadata to JSONL.\n",
    "3.  Inspect a few sample chunks.\n",
    "\n",
    "> **Note**: If you don't have a PDF, you can find many public domain examples online, such as government reports or classic literature from Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65e5ae84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14905483b1d4562a02a082786e7f5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing documents:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87ed41923e24dca90a3c205ab7862ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting Efficient_management_and_compliance_check_of_HVAC_.pdf:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote 113 chunks from 1 docs to data\\outputs\\all_chunks.jsonl\n",
      "{\n",
      "  \"docs\": [\n",
      "    {\n",
      "      \"doc_id\": \"Efficient_management_and_compliance_check_of_HVAC_\",\n",
      "      \"source_path\": \"data\\\\pdfs\\\\Efficient_management_and_compliance_check_of_HVAC_.pdf\",\n",
      "      \"stats\": {\n",
      "        \"pages_total\": 31,\n",
      "        \"pages_scanned\": 0,\n",
      "        \"chunks_total\": 113,\n",
      "        \"avg_chars_per_chunk\": 1025.787610619469,\n",
      "        \"duration_s\": 0.23\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"total_chunks\": 113,\n",
      "  \"total_docs\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# To run this cell, make sure you have at least one PDF in your INPUT_DIR\n",
    "if not any(INPUT_DIR.iterdir()):\n",
    "    print(f\"⚠️ INPUT_DIR '{INPUT_DIR}' is empty. Please add at least one PDF file to run the ingestion task.\")\n",
    "else:\n",
    "    # Ingest the entire directory\n",
    "    chunks, summary = ingest_directory(INPUT_DIR, per_doc_output=True, ocr_scanned=False)\n",
    "    \n",
    "    # Save the combined results\n",
    "    out_path = OUTPUT_DIR / \"all_chunks.jsonl\"\n",
    "    save_jsonl(chunks, out_path)\n",
    "    \n",
    "    print(f\"\\nWrote {summary['total_chunks']} chunks from {summary['total_docs']} docs to {out_path}\")\n",
    "    print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b1cd98",
   "metadata": {},
   "source": [
    "## Inspect sample chunks\n",
    "- View first few chunks with metadata.\n",
    "- Search for a keyword and view matching chunks (quick sanity check)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e40e4a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- First 5 Chunks ---\n",
      "id=Efficient_management_and_compliance_check_of_HVAC__p1_0, page=1, section='A document-centric AEC industry'\n",
      "Semantic Web -1 (2024) 1–31 1 DOI 10.3233/SW-243595 IOS Press CORRECTED PROOF Efﬁcient management and compliance check of HVAC information in the building design phase using Semantic Web technologies ...\n",
      "--------------------------------------------------------------------------------\n",
      "id=Efficient_management_and_compliance_check_of_HVAC__p1_1, page=1, section='A document-centric AEC industry'\n",
      " building services and HVAC components. The Flow Systems Ontology was recently proposed to address this need, but it does not include HVAC components’ size and capacity-related properties. Also, despi...\n",
      "--------------------------------------------------------------------------------\n",
      "id=Efficient_management_and_compliance_check_of_HVAC__p1_2, page=1, section='A document-centric AEC industry'\n",
      "d Air Conditioning (HVAC), SHACL, Semantic Web technologies, Linked Data, compliance checking, SPARQL 1. Introduction 1.1. A document-centric AEC industry Architecture, Engineering and Construction (A...\n",
      "--------------------------------------------------------------------------------\n",
      "id=Efficient_management_and_compliance_check_of_HVAC__p2_3, page=2, section='Linked Data & Semantic Web'\n",
      "2 A. Kücükavci et al. / Efﬁcient management and compliance check of HVAC information experienced improvements in coordination and communication between project stakeholders and digital tools. The BIM ...\n",
      "--------------------------------------------------------------------------------\n",
      "id=Efficient_management_and_compliance_check_of_HVAC__p2_4, page=2, section='Linked Data & Semantic Web'\n",
      " live access is available. The Industry Foundation Classes (IFC) is currently the standard format of building information and has been applied to exchange the needed information among stakeholders, ma...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Found 0 hits for 'agreement' ---\n"
     ]
    }
   ],
   "source": [
    "def find_chunks(keyword: str, chunk_list: list, limit: int = 5):\n",
    "    hits = []\n",
    "    for r in chunk_list:\n",
    "        if keyword.lower() in r[\"text\"].lower():\n",
    "            hits.append(r)\n",
    "        if len(hits) >= limit:\n",
    "            break\n",
    "    return hits\n",
    "\n",
    "if 'chunks' in locals() and chunks:\n",
    "    sample = chunks[:5]\n",
    "    print(\"--- First 5 Chunks ---\")\n",
    "    for r in sample:\n",
    "        print(f\"id={r['id']}, page={r['metadata']['page']}, section='{r['metadata']['section_title']}'\")\n",
    "        print(r[\"text\"][:200].replace(\"\\n\", \" \") + \"...\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    keyword = \"agreement\" # Change this keyword to search for something relevant in your doc\n",
    "    hits = find_chunks(keyword, chunks, limit=3)\n",
    "    print(f\"\\n--- Found {len(hits)} hits for '{keyword}' ---\")\n",
    "    for r in hits:\n",
    "        print(f\"id={r['id']} | page={r['metadata']['page']} | section='{r['metadata']['section_title']}'\")\n",
    "        print(\"...\" + r[\"text\"][:250].replace(\"\\n\", \" \") + \"...\")\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"No chunks found. Please run the ingestion cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d4f0c",
   "metadata": {},
   "source": [
    "## Efficiency notes for 100+ page PDFs\n",
    "- Use `page.get_text(\"text\")` for speed; only switch to OCR for scanned pages and only if needed.\n",
    "- Avoid storing heavy page objects; collect text then release page references.\n",
    "- Strip headers/footers to reduce repeated noise.\n",
    "- Chunk with overlap to preserve context across boundaries.\n",
    "- For very large corpora, write per-document JSONL during processing to limit memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eacbf9",
   "metadata": {},
   "source": [
    "## Output schema (for Module 3)\n",
    "Each JSONL line is a chunk with the following structure:\n",
    "```json\n",
    "{\n",
    "  \"id\": \"string\",\n",
    "  \"text\": \"string\",\n",
    "  \"metadata\": {\n",
    "    \"doc_id\": \"string\",\n",
    "    \"source_path\": \"string\",\n",
    "    \"page\": \"int\",\n",
    "    \"char_start\": \"int\",\n",
    "    \"char_end\": \"int\",\n",
    "    \"section_title\": \"string | null\",\n",
    "    \"section_path\": \"list[string]\",\n",
    "    \"scanned\": \"bool\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "Module 3 will read these JSONL files, generate embeddings, index with FAISS, and enable semantic search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a677cf",
   "metadata": {},
   "source": [
    "## Appendix: Troubleshooting\n",
    "- **If pages come back empty but not scanned**: some PDFs embed text as vector shapes; try `page.get_text(\"rawdict\")` or `\"blocks\"` to diagnose.\n",
    "- **If TOC is missing**: `section_title`/`path` will be `None`/`[]`; consider custom heading detection using `page.get_text(\"dict\")` and analyzing font sizes.\n",
    "- **OCR quality**: install language packs for Tesseract and tune DPI (200–300). Higher DPI → better OCR but slower processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DocumentAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
