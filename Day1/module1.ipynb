{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 — Understanding Document AI & MCP Flow\n",
    "## Objectives\n",
    "- Understand what **Document Intelligence** is and common real-world use cases. 🧠\n",
    "- Learn the idea of **MCP-style workflows** for multi-step AI pipelines.\n",
    "- See the role of tools you'll use later: **LangChain, Gemini API, FAISS, and Tesseract OCR**.\n",
    "- Run a small, end-to-end “MCP Flow” skeleton you'll extend in Modules 2 and 3. 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Document Intelligence?\n",
    "- The practice of automatically **parsing, understanding, and acting** on information in documents (PDFs, images, scans, HTML).\n",
    "- Goes beyond raw text extraction to include **structure** (headings, tables, figures), **semantics** (entities, relations), and **reasoning** (summaries, answers, compliance checks).\n",
    "\n",
    "### Real-world use cases\n",
    "- **Contracts**: clause extraction, risk flags, change tracking, negotiation assistants.\n",
    "- **Research papers**: literature mapping, citation graphing, key findings extraction.\n",
    "- **Manuals/SOPs**: task-aware search, troubleshooting assistants, training content generation.\n",
    "- **Invoices/receipts**: field extraction, validation, and accounting workflows.\n",
    "- **Healthcare**: de-identification, clinical fact extraction, coding support (with strict compliance).\n",
    "\n",
    "### Quality pillars\n",
    "- **Accuracy** (OCR quality, parsing fidelity)\n",
    "- **Traceability** (citations, page refs)\n",
    "- **Latency/cost** (chunking, caching)\n",
    "- **Safety/Compliance** (PII, PHI handling)\n",
    "- **Maintainability** (clear, observable pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCP-style Workflows: Why and How\n",
    "\n",
    "We'll use **MCP** here to mean **“Model-Context-Protocol-style”** workflows: modular, tool-driven pipelines that pass context through well-defined steps (load → parse → chunk → embed → index → retrieve → reason → output).\n",
    "\n",
    "### Benefits\n",
    "- **Composability**: Swap implementations per step without breaking the pipeline.\n",
    "- **Observability**: Log and inspect each stage (inputs, outputs, metadata).\n",
    "- **Persistence**: Save intermediate artifacts (chunks, embeddings, indexes).\n",
    "- **Idempotence**: Re-run safely on the same inputs.\n",
    "- **Reusability**: The same flow works for contracts or research papers with different adapters.\n",
    "\n",
    "### Orchestration options\n",
    "- **LangChain/LangGraph**: chain and graph-based control, memory, tools, retrievers.\n",
    "- **Custom orchestrators**: simple Python runners (we'll build a minimal one here).\n",
    "\n",
    "### Where tools fit\n",
    "- **Parsing**: PyMuPDF (PDF text + layout), Tesseract OCR (for scanned PDFs/images).\n",
    "- **Embeddings/Index**: `sentence-transformers` or Gemini Embeddings → FAISS for similarity search.\n",
    "- **Reasoning**: Gemini API or other LLMs via LangChain for Q&A, summaries, extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "- This module has no hard dependencies on APIs. You can optionally set up Gemini for later modules.\n",
    "- Tesseract is a native binary; we'll provide install tips but not use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.9 (main, Mar 17 2025, 21:06:20) [MSC v.1943 64 bit (AMD64)]\n",
      "Module 1 setup complete (no external API required).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import hashlib\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Callable\n",
    "\n",
    "print(f\"Python {sys.version}\")\n",
    "print(\"Module 1 setup complete (no external API required).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Configure API keys for later modules (not used in Module 1)\n",
    "- **Gemini**: `export GOOGLE_API_KEY` in your environment.\n",
    "- **Tesseract OCR** is a native dependency:\n",
    "  - macOS (Homebrew): `brew install tesseract`\n",
    "  - Ubuntu/Debian: `sudo apt-get install tesseract-ocr`\n",
    "  - Windows: install from https://github.com/tesseract-ocr/tesseract\n",
    "\n",
    "We'll verify environment variables if you want to pre-configure (safe to skip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gemini API key detected in environment (ready for later modules).\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"\")\n",
    "if GOOGLE_API_KEY:\n",
    "    print(\"✅ Gemini API key detected in environment (ready for later modules).\")\n",
    "else:\n",
    "    print(\"⚠️ Gemini API key not set (OK for Module 1).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools Overview\n",
    "- **LangChain**: Toolkit to build RAG, agents, and multi-step pipelines. Provides document loaders, chunkers, retrievers, vector store adapters, and LLM interfaces.\n",
    "- **Gemini API**: Fast, multimodal LLMs and embeddings; great for reasoning and document tasks. We'll use it later for Q&A and summarization.\n",
    "- **FAISS**: High-performance vector similarity search index. We'll integrate it for semantic search in Module 3.\n",
    "- **Tesseract OCR**: Extracts text from scanned PDFs and images; often used alongside PyMuPDF to handle both digital and scanned pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal MCP-style pipeline skeleton\n",
    "\n",
    "We'll model a document pipeline as a sequence of steps. Each step:\n",
    "- Declares a name\n",
    "- Reads/writes to a shared `context` object\n",
    "- Is pure and idempotent (given the same inputs, it produces the same outputs)\n",
    "\n",
    "#### Steps in this demo:\n",
    "1) `load_source`: Simulates loading a document (string + metadata)\n",
    "2) `parse_text`: “Parsing” stage (a no-op here; will be real in Module 2)\n",
    "3) `chunk_text`: Splits into manageable segments (with overlap)\n",
    "4) `embed_stub`: Creates toy embeddings (hash trick) for demo\n",
    "5) `index_stub`: Builds a simple in-memory index and search (cosine similarity)\n",
    "\n",
    "We'll log each step's inputs/outputs to show observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PipelineContext:\n",
    "    # Raw source\n",
    "    source_name: Optional[str] = None\n",
    "    source_bytes: Optional[bytes] = None\n",
    "    source_text: Optional[str] = None\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    # Parsed\n",
    "    text: Optional[str] = None\n",
    "\n",
    "    # Chunking\n",
    "    chunks: List[Dict[str, Any]] = field(default_factory=list)  # {id, text, meta}\n",
    "\n",
    "    # Embeddings\n",
    "    vectors: Optional[List[List[float]]] = None  # aligned with chunks\n",
    "\n",
    "    # Index/storage (stub)\n",
    "    index: Optional[Dict[str, Any]] = None\n",
    "\n",
    "    # Logs\n",
    "    events: List[Dict[str, Any]] = field(default_factory=list)\n",
    "\n",
    "\n",
    "def log_event(ctx: PipelineContext, step: str, detail: Dict[str, Any]):\n",
    "    ctx.events.append({\n",
    "        \"ts\": time.time(),\n",
    "        \"step\": step,\n",
    "        \"detail\": detail\n",
    "    })\n",
    "\n",
    "\n",
    "def run_step(ctx: PipelineContext, name: str, fn: Callable[[PipelineContext], PipelineContext]) -> PipelineContext:\n",
    "    t0 = time.time()\n",
    "    before = json.dumps({\"keys\": list(ctx.__dict__.keys())})\n",
    "    ctx = fn(ctx)\n",
    "    elapsed = round((time.time() - t0) * 1000, 2)\n",
    "    log_event(ctx, name, {\"elapsed_ms\": elapsed})\n",
    "    print(f\"[{name}] done in {elapsed} ms\")\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 — `load_source`\n",
    "- In real pipelines: read file from disk, cloud storage, or a doc store.\n",
    "- Here: we inject a short sample text and basic metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_source(ctx: PipelineContext) -> PipelineContext:\n",
    "    sample = (\n",
    "        \"Master Service Agreement (MSA)\\n\"\n",
    "        \"1. Scope: This Agreement governs the services provided by Vendor X.\\n\"\n",
    "        \"2. Termination: Either party may terminate with 30 days' notice for convenience.\\n\"\n",
    "        \"3. Confidentiality: Both parties shall protect confidential information.\\n\"\n",
    "        \"4. Liability: Vendor X is not liable for indirect damages.\\n\"\n",
    "    )\n",
    "    ctx.source_name = \"sample_contract.txt\"\n",
    "    ctx.source_text = sample\n",
    "    ctx.metadata.update({\n",
    "        \"doc_type\": \"contract\",\n",
    "        \"business_unit\": \"legal\",\n",
    "        \"language\": \"en\",\n",
    "    })\n",
    "    log_event(ctx, \"load_source\", {\"source_name\": ctx.source_name, \"bytes\": bool(ctx.source_bytes)})\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 — `parse_text`\n",
    "- In Module 2, this will use real parsers (e.g., PyMuPDF for PDFs, OCR for images).\n",
    "- Here: treat `source_text` as already parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(ctx: PipelineContext) -> PipelineContext:\n",
    "    if ctx.source_text:\n",
    "        ctx.text = ctx.source_text\n",
    "    elif ctx.source_bytes:\n",
    "        # Placeholder: decode if bytes; real implementation would inspect format\n",
    "        ctx.text = ctx.source_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
    "    else:\n",
    "        ctx.text = \"\"\n",
    "\n",
    "    log_event(ctx, \"parse_text\", {\"text_len\": len(ctx.text or \"\")})\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 — `chunk_text`\n",
    "- Splits long text into overlapping chunks to help later embeddings and retrieval.\n",
    "- We keep it simple here: split by lines, then merge small lines; add overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(ctx: PipelineContext, max_chars: int = 120, overlap: int = 30) -> PipelineContext:\n",
    "    if not ctx.text:\n",
    "        ctx.chunks = []\n",
    "        log_event(ctx, \"chunk_text\", {\"chunks\": 0})\n",
    "        return ctx\n",
    "\n",
    "    lines = [ln.strip() for ln in ctx.text.splitlines() if ln.strip()]\n",
    "    chunks = []\n",
    "    buf = \"\"\n",
    "    for ln in lines:\n",
    "        if len(buf) + 1 + len(ln) <= max_chars:\n",
    "            buf = (buf + \" \" + ln).strip()\n",
    "        else:\n",
    "            chunks.append(buf)\n",
    "            # create overlap by keeping last N chars\n",
    "            buf = (buf[-overlap:] + \" \" + ln).strip()\n",
    "    if buf:\n",
    "        chunks.append(buf)\n",
    "\n",
    "    ctx.chunks = [\n",
    "        {\"id\": f\"chunk-{i}\", \"text\": ch, \"meta\": {\"order\": i}} for i, ch in enumerate(chunks)\n",
    "    ]\n",
    "    log_event(ctx, \"chunk_text\", {\"chunks\": len(ctx.chunks), \"max_chars\": max_chars, \"overlap\": overlap})\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 — `embed_stub`\n",
    "- Real embeddings (Module 3) will use `sentence-transformers` or Gemini Embeddings.\n",
    "- For Module 1, we'll create a **“hash trick”** embedding:\n",
    "  - Map each word to a bucket using a stable hash.\n",
    "  - Produce a fixed-length vector of counts (L2-normalized).\n",
    "- This is **only for demonstration** and testing the pipeline shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hash_vector(text: str, dim: int = 64) -> List[float]:\n",
    "    vec = [0.0] * dim\n",
    "    for token in text.lower().split():\n",
    "        h = int(hashlib.md5(token.encode(\"utf-8\")).hexdigest(), 16)\n",
    "        vec[h % dim] += 1.0\n",
    "    # L2 normalize\n",
    "    norm = math.sqrt(sum(x*x for x in vec)) or 1.0\n",
    "    return [x / norm for x in vec]\n",
    "\n",
    "def embed_stub(ctx: PipelineContext, dim: int = 64) -> PipelineContext:\n",
    "    vectors = []\n",
    "    for ch in ctx.chunks:\n",
    "        vectors.append(_hash_vector(ch[\"text\"], dim=dim))\n",
    "    ctx.vectors = vectors\n",
    "    log_event(ctx, \"embed_stub\", {\"dim\": dim, \"vectors\": len(vectors)})\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 — `index_stub` and simple search\n",
    "- In Module 3, we'll integrate **FAISS** for efficient vector search.\n",
    "- Here: store vectors in memory and compute cosine similarity at query time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cosine(a: List[float], b: List[float]) -> float:\n",
    "    return sum(x*y for x, y in zip(a, b))\n",
    "\n",
    "def index_stub(ctx: PipelineContext) -> PipelineContext:\n",
    "    if not ctx.vectors or not ctx.chunks:\n",
    "        ctx.index = None\n",
    "        return ctx\n",
    "    ctx.index = {\n",
    "        \"vectors\": ctx.vectors,\n",
    "        \"chunks\": ctx.chunks,\n",
    "        \"dim\": len(ctx.vectors[0]) if ctx.vectors else 0\n",
    "    }\n",
    "    log_event(ctx, \"index_stub\", {\"count\": len(ctx.vectors)})\n",
    "    return ctx\n",
    "\n",
    "def search_stub(ctx: PipelineContext, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "    if not ctx.index:\n",
    "        return []\n",
    "    qv = _hash_vector(query, dim=ctx.index[\"dim\"])\n",
    "    sims = [(_cosine(qv, v), i) for i, v in enumerate(ctx.index[\"vectors\"])]\n",
    "    sims.sort(reverse=True)\n",
    "    results = []\n",
    "    for score, i in sims[:top_k]:\n",
    "        results.append({\n",
    "            \"score\": float(score),\n",
    "            \"chunk\": ctx.index[\"chunks\"][i]\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline\n",
    "\n",
    "We'll execute all steps and then run a simple semantic search query over the stub index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load_source] done in 0.0 ms\n",
      "[parse_text] done in 0.0 ms\n",
      "[chunk_text] done in 0.0 ms\n",
      "[embed_stub] done in 33.5 ms\n",
      "[index_stub] done in 0.0 ms\n",
      "\n",
      "---\n",
      "\n",
      "Chunks created: 4\n",
      "- chunk-0: Master Service Agreement (MSA) 1. Scope: This Agreement governs the services provided by Vendor X.\n",
      "- chunk-1: services provided by Vendor X. 2. Termination: Either party may terminate with 30 days' notice for convenience.\n",
      "- chunk-2: days' notice for convenience. 3. Confidentiality: Both parties shall protect confidential information.\n",
      "- chunk-3: tect confidential information. 4. Liability: Vendor X is not liable for indirect damages.\n"
     ]
    }
   ],
   "source": [
    "ctx = PipelineContext()\n",
    "\n",
    "for name, fn in [\n",
    "    (\"load_source\", load_source),\n",
    "    (\"parse_text\", parse_text),\n",
    "    (\"chunk_text\", chunk_text),\n",
    "    (\"embed_stub\", embed_stub),\n",
    "    (\"index_stub\", index_stub),\n",
    "]:\n",
    "    ctx = run_step(ctx, name, fn)\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "print(f\"Chunks created: {len(ctx.chunks)}\")\n",
    "for ch in ctx.chunks:\n",
    "    print(f\"- {ch['id']}: {ch['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a query (semantic-ish search on the stub index)\n",
    "Example queries:\n",
    "- `termination notice`\n",
    "- `confidentiality information`\n",
    "- `indirect damages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'termination notice'\n",
      "  score=0.548 | chunk-3 | tect confidential information. 4. Liability: Vendor X is not liable for indirect damages.\n",
      "  score=0.189 | chunk-2 | days' notice for convenience. 3. Confidentiality: Both parties shall protect confidential information.\n",
      "\n",
      "Query: 'confidentiality information'\n",
      "  score=0.365 | chunk-3 | tect confidential information. 4. Liability: Vendor X is not liable for indirect damages.\n",
      "  score=0.162 | chunk-1 | services provided by Vendor X. 2. Termination: Either party may terminate with 30 days' notice for convenience.\n",
      "\n",
      "Query: 'indirect damages'\n",
      "  score=0.183 | chunk-3 | tect confidential information. 4. Liability: Vendor X is not liable for indirect damages.\n",
      "  score=0.162 | chunk-1 | services provided by Vendor X. 2. Termination: Either party may terminate with 30 days' notice for convenience.\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"termination notice\",\n",
    "    \"confidentiality information\",\n",
    "    \"indirect damages\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    results = search_stub(ctx, q, top_k=2)\n",
    "    print(f\"\\nQuery: '{q}'\")\n",
    "    for r in results:\n",
    "        print(f\"  score={r['score']:.3f} | {r['chunk']['id']} | {r['chunk']['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observability: Inspect the event log\n",
    "\n",
    "The log shows each step completion and timing. In production, you'd also capture inputs/outputs, trace IDs, errors, and persist artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pipeline Event Log ---\n",
      "19:29:29 | load_source  | {'source_name': 'sample_contract.txt', 'bytes': False}\n",
      "19:29:29 | load_source  | {'elapsed_ms': 0.0}\n",
      "19:29:29 | parse_text   | {'text_len': 312}\n",
      "19:29:29 | parse_text   | {'elapsed_ms': 0.0}\n",
      "19:29:29 | chunk_text   | {'chunks': 4, 'max_chars': 120, 'overlap': 30}\n",
      "19:29:29 | chunk_text   | {'elapsed_ms': 0.0}\n",
      "19:29:29 | embed_stub   | {'dim': 64, 'vectors': 4}\n",
      "19:29:29 | embed_stub   | {'elapsed_ms': 33.5}\n",
      "19:29:29 | index_stub   | {'count': 4}\n",
      "19:29:29 | index_stub   | {'elapsed_ms': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Pipeline Event Log ---\")\n",
    "for e in ctx.events:\n",
    "    print(f\"{time.strftime('%H:%M:%S', time.localtime(e['ts']))} | {e['step']:<12} | {e['detail']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Preview: How this grows in Modules 2 and 3\n",
    "- **Module 2 (Parsing Large PDFs)**:\n",
    "  - Replace `load_source`/`parse_text` with PyMuPDF-based PDF loader and text extractor.\n",
    "  - Add page-level metadata (page number, section headers).\n",
    "  - Robust chunking with hierarchy and overlaps; support multiple PDFs.\n",
    "- **Module 3 (Embeddings & Vector Store)**:\n",
    "  - Replace `embed_stub` with real embeddings (`all-MiniLM-L6-v2` or Gemini Embeddings).\n",
    "  - Replace `index_stub`/`search_stub` with FAISS index and retrieval.\n",
    "  - Persist vector DB locally for reuse.\n",
    "\n",
    "### Orchestration Options:\n",
    "- Keep the simple runner pattern and add caching/persistence.\n",
    "- Move to LangChain/LangGraph for tool calls, memory, and branching logic (e.g., OCR fallback if page has no text).\n",
    "\n",
    "### Notes on Tesseract OCR:\n",
    "- Use when PDFs are scanned images; run OCR per page and merge with PyMuPDF outputs.\n",
    "- Validate results with confidence scores; consider language packs and layout-aware OCR for better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Minimal LangChain-style conceptual mapping (no API calls)\n",
    "- `Loader`: converts bytes/files → `Documents` with metadata\n",
    "- `TextSplitter`: produces chunks with overlap\n",
    "- `Embeddings`: transforms chunks → vectors\n",
    "- `VectorStore`: builds index (e.g., FAISS) and provides retriever\n",
    "- `Chain/Agent`: uses retriever + LLM to answer questions with citations\n",
    "\n",
    "In Module 3 we'll replace the stub embedding and index with actual LangChain components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Module 1\n",
    "You now have:\n",
    "- A clear mental model of Document Intelligence and MCP-style flows. ✅\n",
    "- A small, observable pipeline skeleton ready to be upgraded in Modules 2 and 3. ✅\n",
    "\n",
    "### Next: Module 2 — Parsing Large PDFs Efficiently\n",
    "- PyMuPDF extraction\n",
    "- Smarter chunking with overlaps and hierarchy\n",
    "- Multi-PDF support and metadata persistence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DocumentAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
