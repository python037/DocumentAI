{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 â€” Embeddings & Vector Store (FAISS)\n",
    "\n",
    "## Objectives\n",
    "- Understand embeddings and **semantic search**. ðŸ’¡\n",
    "- Generate embeddings with:\n",
    "  - **Local**: `sentence-transformers/all-MiniLM-L6-v2` (free, CPU-friendly).\n",
    "  - **Cloud**: Gemini `text-embedding-004` (optional, quota-limited).\n",
    "- Build and query a **FAISS vector index** (cosine similarity).\n",
    "- **Persist** the index and metadata locally and reload it.\n",
    "- Optional: **MMR re-ranking**; optional answer synthesis with Gemini. ðŸ¤–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Run **Module 2** and produce JSONL chunks in `data/outputs`.\n",
    "- For Gemini embeddings/answering: set `GOOGLE_API_KEY` in your environment.\n",
    "\n",
    "### Install dependencies if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ee3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q faiss-cpu sentence-transformers tqdm\n",
    "!{sys.executable} -m install -q google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q faiss-cpu sentence-transformers tqdm\n",
    "\n",
    "# Optional for Gemini\n",
    "# %pip install -q google-generativeai\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Handles environments where module name differs\n",
    "try:\n",
    "    import faiss  # noqa: F401\n",
    "except ImportError:\n",
    "    import faiss_cpu as faiss  # type: ignore\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"FAISS: {faiss.get_version_string() if hasattr(faiss, 'get_version_string') else 'ok'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "- `INPUT_CHUNKS`: path to Module 2 output JSONL (combined or per-doc).\n",
    "- `EMBEDDING_BACKEND`: `\"minilm\"` (local) or `\"gemini\"` (cloud).\n",
    "- `INDEX_DIR`: folder to persist FAISS index and metadata.\n",
    "- `BATCH_SIZE`: adjust to trade speed vs memory.\n",
    "- `MMR`: optional re-ranking for diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CHUNKS = Path(\"./data/outputs/all_chunks.jsonl\")   # change if needed\n",
    "INDEX_DIR = Path(\"./data/indexes/my_corpus\")\n",
    "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EMBEDDING_BACKEND = \"minilm\"  # \"minilm\" or \"gemini\"\n",
    "BATCH_SIZE = 64\n",
    "MAX_TEXT_LEN = 4000  # char limit per chunk for embedding API safety\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load chunks from Module 2\n",
    "\n",
    "Expects JSONL lines with:\n",
    "`id`, `text`, `metadata: {doc_id, page, section_title, section_path, ...}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    items = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                items.append(json.loads(line))\n",
    "    return items\n",
    "\n",
    "assert INPUT_CHUNKS.exists(), f\"Missing chunks file: {INPUT_CHUNKS}. Please run Module 2 first.\"\n",
    "chunks = load_jsonl(INPUT_CHUNKS)\n",
    "print(f\"Loaded {len(chunks)} chunks from {INPUT_CHUNKS}\")\n",
    "\n",
    "print(\"\\n--- Inspecting a sample ---\")\n",
    "for r in chunks[:3]:\n",
    "    print(r[\"id\"], \"| page:\", r[\"metadata\"].get(\"page\"), \"| section:\", r[\"metadata\"].get(\"section_title\"))\n",
    "    print(r[\"text\"][:140].replace(\"\\n\",\" \"), \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding backends\n",
    "- **MiniLM (local)**: 384-dim, fast on CPU, no API cost.\n",
    "- **Gemini (cloud)**: `text-embedding-004` (768-dim), requires `GOOGLE_API_KEY`.\n",
    "\n",
    "Weâ€™ll normalize embeddings to unit length and use FAISS `IndexFlatIP` (inner product) to emulate cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniLMBackend:\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        import torch\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "        self.model_name = model_name\n",
    "        self.dim = self.model.get_sentence_embedding_dimension()\n",
    "\n",
    "    def encode(self, texts: List[str], batch_size: int = 64) -> np.ndarray:\n",
    "        # SentenceTransformer returns np.ndarray float32 by default\n",
    "        return self.model.encode(texts, batch_size=batch_size, convert_to_numpy=True, normalize_embeddings=False)\n",
    "\n",
    "class GeminiBackend:\n",
    "    def __init__(self, model_name: str = \"models/text-embedding-004\", api_key: Optional[str] = None):\n",
    "        assert api_key, \"GOOGLE_API_KEY is required for Gemini embeddings.\"\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.genai = genai\n",
    "        self.model_name = model_name\n",
    "        self.dim = 768  # text-embedding-004 output dimension\n",
    "\n",
    "    def encode(self, texts: List[str], batch_size: int = 100) -> np.ndarray:\n",
    "        vecs: List[List[float]] = []\n",
    "        # Use the batch embedding API for efficiency\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Gemini embedding\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            # Truncate texts in the batch if necessary\n",
    "            batch_truncated = [t[:MAX_TEXT_LEN] if MAX_TEXT_LEN and len(t) > MAX_TEXT_LEN else t for t in batch]\n",
    "            resp = self.genai.embed_content(model=self.model_name, content=batch_truncated, task_type=\"retrieval_document\")\n",
    "            vecs.extend(resp[\"embedding\"])\n",
    "        return np.asarray(vecs, dtype=np.float32)\n",
    "\n",
    "def get_backend(name: str):\n",
    "    if name.lower() == \"minilm\":\n",
    "        be = MiniLMBackend()\n",
    "        print(f\"Using MiniLM backend: {be.model_name}, dim={be.dim}\")\n",
    "        return be\n",
    "    elif name.lower() == \"gemini\":\n",
    "        be = GeminiBackend(api_key=GOOGLE_API_KEY)\n",
    "        print(f\"Using Gemini backend: {be.model_name}, dim={be.dim}\")\n",
    "        return be\n",
    "    else:\n",
    "        raise ValueError(\"EMBEDDING_BACKEND must be 'minilm' or 'gemini'.\")\n",
    "\n",
    "backend = get_backend(EMBEDDING_BACKEND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare texts and compute embeddings\n",
    "- Keep a `docstore` mapping array aligned with vectors: each entry holds `id`, `text`, and `metadata` for retrieval/citations.\n",
    "- Normalize embeddings to unit length for cosine search via inner product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_texts(records: List[Dict[str, Any]]) -> List[str]:\n",
    "    \"\"\"Prepares text for embedding, applying truncation if necessary.\"\"\"\n",
    "    texts = []\n",
    "    for r in records:\n",
    "        t = r[\"text\"].strip()\n",
    "        texts.append(t)\n",
    "    return texts\n",
    "\n",
    "def l2_normalize(vecs: np.ndarray) -> np.ndarray:\n",
    "    norms = np.linalg.norm(vecs, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1.0\n",
    "    return vecs / norms\n",
    "\n",
    "texts = prepare_texts(chunks)\n",
    "print(f\"Embedding {len(texts)} chunks...\")\n",
    "\n",
    "emb = backend.encode(texts, batch_size=BATCH_SIZE).astype(np.float32)\n",
    "assert emb.shape[1] == backend.dim, f\"Dimension mismatch: got {emb.shape[1]}, expected {backend.dim}\"\n",
    "emb = l2_normalize(emb)\n",
    "\n",
    "print(f\"Embeddings shape: {emb.shape}\")\n",
    "\n",
    "# Build docstore aligned with embeddings\n",
    "docstore = [\n",
    "    {\n",
    "        \"id\": r[\"id\"],\n",
    "        \"text\": r[\"text\"],\n",
    "        \"metadata\": r.get(\"metadata\", {})\n",
    "    }\n",
    "    for r in chunks\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build FAISS index (cosine similarity via inner product)\n",
    "- `IndexFlatIP` with L2-normalized vectors.\n",
    "- Persist index, vectors (optional for MMR), and docstore to disk.\n",
    "- Store a small manifest to ensure consistent reloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faiss_index(vectors: np.ndarray) -> faiss.Index:\n",
    "    dim = vectors.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(vectors)\n",
    "    return index\n",
    "\n",
    "def save_index(index: faiss.Index,\n",
    "               vectors: np.ndarray,\n",
    "               docstore: List[Dict[str, Any]],\n",
    "               backend_name: str,\n",
    "               dim: int,\n",
    "               out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    faiss.write_index(index, str(out_dir / \"index.faiss\"))\n",
    "    # Optional: save vectors for MMR re-ranking and reproducibility\n",
    "    np.save(out_dir / \"vectors.npy\", vectors)\n",
    "    \n",
    "    # Save docstore and manifest\n",
    "    with (out_dir / \"docstore.jsonl\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for d in docstore:\n",
    "            f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "    manifest = {\n",
    "        \"backend\": backend_name,\n",
    "        \"dim\": dim,\n",
    "        \"count\": int(vectors.shape[0]),\n",
    "        \"created_at\": time.time()\n",
    "    }\n",
    "    with (out_dir / \"manifest.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(manifest, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_index(in_dir: Path) -> Tuple[faiss.Index, np.ndarray, List[Dict[str, Any]], Dict[str, Any]]:\n",
    "    index = faiss.read_index(str(in_dir / \"index.faiss\"))\n",
    "    vectors = np.load(in_dir / \"vectors.npy\")\n",
    "    docstore = load_jsonl(in_dir / \"docstore.jsonl\")\n",
    "    manifest = json.loads((in_dir / \"manifest.json\").read_text(encoding=\"utf-8\"))\n",
    "    return index, vectors, docstore, manifest\n",
    "\n",
    "# Build and save\n",
    "index = build_faiss_index(emb)\n",
    "save_index(index, emb, docstore, EMBEDDING_BACKEND, backend.dim, INDEX_DIR)\n",
    "print(f\"Index saved to {INDEX_DIR}\")\n",
    "\n",
    "# Reload to verify\n",
    "index2, vectors2, store2, manifest2 = load_index(INDEX_DIR)\n",
    "print(f\"Reloaded index: dim={manifest2['dim']}, count={manifest2['count']}, backend={manifest2['backend']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search utilities\n",
    "- `search_faiss`: pure FAISS top-k search.\n",
    "- `mmr_rerank`: optional **Maximal Marginal Relevance** reranking using stored vectors.\n",
    "- `search`: convenience wrapper with optional MMR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(q: str, backend) -> np.ndarray:\n",
    "    q = q.strip()\n",
    "    qv = backend.encode([q], batch_size=1).astype(np.float32)\n",
    "    qv = l2_normalize(qv)\n",
    "    return qv\n",
    "\n",
    "def search_faiss(index: faiss.Index, query_vec: np.ndarray, top_k: int = 5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    D, I = index.search(query_vec, top_k)\n",
    "    return D[0], I[0]\n",
    "\n",
    "def mmr_rerank(query_vec: np.ndarray,\n",
    "               candidates_idx: np.ndarray,\n",
    "               candidate_vecs: np.ndarray,\n",
    "               top_k: int = 5,\n",
    "               lambda_mult: float = 0.5) -> List[int]:\n",
    "    # Simple MMR: select items maximizing lambda*sim(query,doc) - (1-lambda)*max_sim_to_selected\n",
    "    selected = []\n",
    "    candidate_list = candidates_idx.tolist()\n",
    "    \n",
    "    # Precompute similarity of all candidates to the query\n",
    "    sims_to_query = (candidate_vecs @ query_vec[0])\n",
    "    \n",
    "    while len(selected) < min(top_k, len(candidate_list)):\n",
    "        best_candidate = -1\n",
    "        best_score = -np.inf\n",
    "        \n",
    "        for idx in candidate_list:\n",
    "            doc_sim_to_query = sims_to_query[idx]\n",
    "            if not selected:\n",
    "                score = doc_sim_to_query\n",
    "            else:\n",
    "                max_sim_to_selected = max(candidate_vecs[idx] @ candidate_vecs[j] for j in selected)\n",
    "                score = lambda_mult * doc_sim_to_query - (1 - lambda_mult) * max_sim_to_selected\n",
    "                \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_candidate = idx\n",
    "                \n",
    "        if best_candidate != -1:\n",
    "            selected.append(best_candidate)\n",
    "            candidate_list.remove(best_candidate)\n",
    "            \n",
    "    return selected\n",
    "\n",
    "def search(query: str,\n",
    "           index: faiss.Index,\n",
    "           backend,\n",
    "           store: List[Dict[str, Any]],\n",
    "           vectors: Optional[np.ndarray] = None,\n",
    "           top_k: int = 5,\n",
    "           use_mmr: bool = False,\n",
    "           mmr_lambda: float = 0.5,\n",
    "           fetch_k: int = 25) -> List[Dict[str, Any]]:\n",
    "    \n",
    "    qv = embed_query(query, backend)\n",
    "    D, I = search_faiss(index, qv, top_k=fetch_k if use_mmr and vectors is not None else top_k)\n",
    "    \n",
    "    if use_mmr and vectors is not None:\n",
    "        selected_indices = mmr_rerank(qv, I, vectors, top_k=top_k, lambda_mult=mmr_lambda)\n",
    "        # Recalculate scores and indices for the final top_k\n",
    "        final_I = np.array(selected_indices, dtype=int)\n",
    "        final_D = np.array([(vectors[i] @ qv[0]) for i in final_I], dtype=np.float32)\n",
    "    else:\n",
    "        final_I, final_D = I, D\n",
    "        \n",
    "    results = []\n",
    "    for score, idx in zip(final_D[:top_k], final_I[:top_k]):\n",
    "        rec = store[idx]\n",
    "        results.append({\n",
    "            \"score\": float(score),\n",
    "            \"id\": rec[\"id\"],\n",
    "            \"text\": rec[\"text\"],\n",
    "            \"metadata\": rec[\"metadata\"]\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try some queries\n",
    "\n",
    "Use examples relevant to your PDFs. For the sample from Module 1/2: `\"termination\"`, `\"confidentiality\"`, `\"indirect damages\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"termination notice\",\n",
    "    \"confidentiality obligations\",\n",
    "    \"indirect damages liability\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\n--- Query: '{q}' (with MMR) ---\")\n",
    "    res = search(q, index2, backend, store2, vectors=vectors2, top_k=3, use_mmr=True, mmr_lambda=0.5)\n",
    "    for r in res:\n",
    "        m = r[\"metadata\"]\n",
    "        cite = f\"{m.get('doc_id','?')} p.{m.get('page','?')}\"\n",
    "        print(f\"  score={r['score']:.3f} | {cite} | section: {m.get('section_title')}\")\n",
    "        print(\"    \" + r[\"text\"][:220].replace(\"\\n\", \" \") + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Task\n",
    "1)  Point `INPUT_CHUNKS` to your Module 2 JSONL.\n",
    "2)  Choose `EMBEDDING_BACKEND = \"minilm\"` (or `\"gemini\"` with `GOOGLE_API_KEY` set).\n",
    "3)  Run embedding â†’ indexing â†’ search.\n",
    "4)  Inspect top hits and verify they make sense with citations (`doc_id`/`page`/`section`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Answer synthesis with Gemini\n",
    "- Retrieve top-k chunks, then ask Gemini to compose a concise answer citing pages.\n",
    "- Requires `GOOGLE_API_KEY`.\n",
    "- Keep prompts small; pass only the needed chunk texts to control cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GEMINI_FOR_ANSWERS = bool(GOOGLE_API_KEY) and EMBEDDING_BACKEND == 'gemini'\n",
    "\n",
    "def answer_with_gemini(query: str, retrieved: List[Dict[str, Any]]) -> str:\n",
    "    assert GOOGLE_API_KEY, \"GOOGLE_API_KEY not set.\"\n",
    "    import google.generativeai as genai\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "    \n",
    "    ctx_lines = []\n",
    "    for r in retrieved:\n",
    "        m = r[\"metadata\"]\n",
    "        cite = f\"{m.get('doc_id','?')} p.{m.get('page','?')}\"\n",
    "        ctx_lines.append(f\"Source [{cite}]:\\n{r['text']}\")\n",
    "    context = \"\\n\\n\".join(ctx_lines)\n",
    "    \n",
    "    prompt = (\n",
    "        \"You are a helpful assistant. Use only the provided context to answer the question. \"\n",
    "        \"Your answer must be concise and grounded in the provided text. \"\n",
    "        \"Cite your sources in brackets like [doc_id p.page]. If the information is not in the context, say you don't know.\\n\\n\"\n",
    "        f\"-- CONTEXT --\\n{context}\\n\\n-- QUESTION --\\n{query}\\n\\n-- ANSWER --\"\n",
    "    )\n",
    "    \n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    resp = model.generate_content(prompt)\n",
    "    return resp.text\n",
    "\n",
    "if USE_GEMINI_FOR_ANSWERS:\n",
    "    q = \"What are the termination conditions in this agreement?\"\n",
    "    print(f\"\\n--- Answering with Gemini: '{q}' ---\")\n",
    "    top = search(q, index2, backend, store2, vectors=vectors2, top_k=5, use_mmr=True)\n",
    "    ans = answer_with_gemini(q, top)\n",
    "    print(ans)\n",
    "else:\n",
    "    print(\"\\nGemini answering disabled (GOOGLE_API_KEY not set or backend is not 'gemini').\")\n",
    "    print(\"Retrieval results above can guide manual inspection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the index with new chunks\n",
    "- You can append new documents without rebuilding from scratch (`IndexFlatIP` supports `add`).\n",
    "- Remember to:\n",
    "  - Embed new chunks with the **same backend**.\n",
    "  - L2-normalize embeddings.\n",
    "  - Append to FAISS, `vectors.npy`, and `docstore.jsonl`.\n",
    "  - Update manifest count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_chunks_to_index(new_chunks_path: Path,\n",
    "                        index_dir: Path,\n",
    "                        backend) -> None:\n",
    "    # Load existing\n",
    "    index, vectors, store, manifest = load_index(index_dir)\n",
    "    \n",
    "    # Load new chunks\n",
    "    new_records = load_jsonl(new_chunks_path)\n",
    "    new_texts = prepare_texts(new_records)\n",
    "    if not new_texts:\n",
    "        print(\"No new chunks found.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Adding {len(new_records)} new chunks to index...\")\n",
    "    new_emb = backend.encode(new_texts, batch_size=BATCH_SIZE).astype(np.float32)\n",
    "    new_emb = l2_normalize(new_emb)\n",
    "    \n",
    "    # Add to index and buffers\n",
    "    index.add(new_emb)\n",
    "    vectors = np.concatenate([vectors, new_emb], axis=0)\n",
    "    for r in new_records:\n",
    "        store.append({\"id\": r[\"id\"], \"text\": r[\"text\"], \"metadata\": r.get(\"metadata\", {})})\n",
    "        \n",
    "    # Save back\n",
    "    save_index(index, vectors, store, manifest['backend'], manifest['dim'], index_dir)\n",
    "    print(f\"Index updated: total vectors={index.ntotal}\")\n",
    "\n",
    "# Example (disabled by default)\n",
    "# new_file = Path(\"./data/outputs/new_doc_chunks.jsonl\")\n",
    "# if new_file.exists():\n",
    "#     add_chunks_to_index(new_file, INDEX_DIR, backend)\n",
    "# else:\n",
    "#     print(f\"Skipping index update: '{new_file}' not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical notes\n",
    "- `all-MiniLM-L6-v2` (384-dim) is a great default: small, fast, robust for general semantic search.\n",
    "- Normalize embeddings and use `IndexFlatIP` to emulate cosine similarity.\n",
    "- Persist a manifest including backend name and dim; **avoid mixing different embedding models** in one index.\n",
    "- For 100k+ chunks, consider IVF/HNSW indexes for faster search (e.g., `IndexIVFFlat`), but start with `Flat` for correctness.\n",
    "- Keep `doc_id`/`page`/`section` in docstore for citations and traceability.\n",
    "- MMR re-ranking can improve diversity when top results are near-duplicates.\n",
    "\n",
    "### Next steps\n",
    "- Integrate with a LangChain retriever and QA chain.\n",
    "- Add caching for embeddings (hash chunk text â†’ vector).\n",
    "- Add quality evals: query sets, hit-rate, manual judgments with a small rubric."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DocumentAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
